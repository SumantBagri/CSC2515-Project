{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a7a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "import numpy as np\n",
    "import joblib\n",
    "from catboost.utils import read_cd\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01eb9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(name):\n",
    "    directory = os.path.dirname(name)\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9dea251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_classification_dataset(name):\n",
    "    # converting categorical features to numerical\n",
    "\n",
    "    data_dir = os.path.join('datasets', name)\n",
    "    train_file = os.path.join(data_dir, 'full_train')\n",
    "    test_file = os.path.join(data_dir, 'test')\n",
    "    cd_file = os.path.join(data_dir, 'pool.cd')\n",
    "\n",
    "    train = np.loadtxt(train_file, delimiter=\"\\t\", dtype=\"object\")\n",
    "    test = np.loadtxt(test_file, delimiter=\"\\t\", dtype=\"object\")\n",
    "    cd = read_cd(cd_file, data_file=train_file)\n",
    "\n",
    "    # Target can be called 'Label' or 'Target' in pool.cd\n",
    "    try:\n",
    "        label_ind = cd['column_type_to_indices']['Label']\n",
    "    except:\n",
    "        label_ind = cd['column_type_to_indices']['Target']\n",
    "\n",
    "    np.random.seed(42)  # fix random seed\n",
    "    train = np.random.permutation(train)\n",
    "\n",
    "    y_train = train[:, label_ind]\n",
    "    y_train = y_train.reshape(-1)\n",
    "\n",
    "    y_test = test[:, label_ind]\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    try:\n",
    "        cat_features = cd['column_type_to_indices']['Categ']  # features to be replaced\n",
    "\n",
    "        enc = LeaveOneOutEncoder(cols=cat_features, return_df=False, random_state=10, sigma=0.3)\n",
    "    except:\n",
    "        enc = LeaveOneOutEncoder(return_df=False, random_state=10, sigma=0.3)\n",
    "\n",
    "    transformed_train = enc.fit_transform(train, y_train).astype(\"float64\")\n",
    "    X_train = np.delete(transformed_train, label_ind, 1)  # remove target column\n",
    "\n",
    "    transformed_test = enc.transform(test).astype(\"float64\")\n",
    "    X_test = np.delete(transformed_test, label_ind, 1)  # remove target column\n",
    "\n",
    "    return np.nan_to_num(X_train), y_train, np.nan_to_num(X_test), y_test, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32940070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb86cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rf_ensemble_classification(dataset_name, num_models=10, \n",
    "                                        n_estimators = 1000, compress=3, \n",
    "                                        n_jobs=-1, max_depth=10):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, _ = process_classification_dataset(dataset_name)\n",
    "\n",
    "   \n",
    "    depths = [5,10,15,20]\n",
    "    estimators = [1000,3000,5000]\n",
    "    seed = 0\n",
    "    param_grid = { \n",
    "    'n_estimators': [1000, 3000, 5000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [5,10,15,20],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "    }\n",
    "    rf_model = RandomForestClassifier(random_state=seed)\n",
    "    for i in range(num_models):\n",
    "        \n",
    "        shape = (len(depths), len(estimators))\n",
    "\n",
    "        results = np.zeros(shape)\n",
    "        CV_rfc = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv= 5)\n",
    "        CV_rfc.fit(X_train, y_train)\n",
    "        best_parameters = CV_rfc.best_params_\n",
    "        print(\"Best Paprameter for i = \", i , \"are \", best_parameters, \"\\n\\n\\n\")\n",
    "\n",
    "        seed += 1 # new seed for each ensemble element\n",
    "        #model.fit(X_train, y_train) \n",
    "        #results[d, n] = model.evals_result_['validation']['Logloss'][-1]\n",
    "        rfc1=RandomForestClassifier(random_state=42, max_features=best_parameters['max_features'], n_estimators= best_parameters['n_estimators'], \n",
    "                                    max_depth=best_parameters['max_depth'], criterion=best_parameters['criterion'])\n",
    "\n",
    "        joblib.dump(rfc1, \"results/models/\" + dataset_name + \"_\" \n",
    "                    + \"rf\" + \"_\" + str(i), compress=compress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b5fbd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = internet\n",
      "training models...\n",
      "Best Paprameter for i =  0 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  1 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  2 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  3 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  4 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  5 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  6 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n",
      "Best Paprameter for i =  7 are  {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 1000} \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging copy.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=12'>13</a>\u001b[0m \u001b[39m# Training all models\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining models...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=14'>15</a>\u001b[0m generate_rf_ensemble_classification(name)\n",
      "\u001b[1;32m/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging copy.ipynb Cell 6\u001b[0m in \u001b[0;36mgenerate_rf_ensemble_classification\u001b[0;34m(dataset_name, num_models, n_estimators, compress, n_jobs, max_depth)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=21'>22</a>\u001b[0m results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(shape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=22'>23</a>\u001b[0m CV_rfc \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mrf_model, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=23'>24</a>\u001b[0m CV_rfc\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=24'>25</a>\u001b[0m best_parameters \u001b[39m=\u001b[39m CV_rfc\u001b[39m.\u001b[39mbest_params_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gurmanbhullar/CSC2515-Project/Codebase/classification_bagging%20copy.ipynb#ch0000004?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Paprameter for i = \u001b[39m\u001b[39m\"\u001b[39m, i , \u001b[39m\"\u001b[39m\u001b[39mare \u001b[39m\u001b[39m\"\u001b[39m, best_parameters, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m         clone(base_estimator),\n\u001b[1;32m    841\u001b[0m         X,\n\u001b[1;32m    842\u001b[0m         y,\n\u001b[1;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    853\u001b[0m )\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:702\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    699\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    701\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m--> 702\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[1;32m    703\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:761\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    759\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    760\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[1;32m    762\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     \u001b[39mif\u001b[39;00m error_score \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:418\u001b[0m, in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_passthrough_scorer\u001b[39m(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    417\u001b[0m     \u001b[39m\"\"\"Function that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mscore(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:651\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 651\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:808\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    788\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    811\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:861\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    856\u001b[0m all_proba \u001b[39m=\u001b[39m [\n\u001b[1;32m    857\u001b[0m     np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    858\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[1;32m    859\u001b[0m ]\n\u001b[1;32m    860\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[0;32m--> 861\u001b[0m Parallel(\n\u001b[1;32m    862\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    863\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    864\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    865\u001b[0m )(\n\u001b[1;32m    866\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39;49mpredict_proba, X, all_proba, lock)\n\u001b[1;32m    867\u001b[0m     \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_\n\u001b[1;32m    868\u001b[0m )\n\u001b[1;32m    870\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[1;32m    871\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:640\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[1;32m    634\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[39m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m     prediction \u001b[39m=\u001b[39m predict(X, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    641\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:976\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    975\u001b[0m     proba \u001b[39m=\u001b[39m proba[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_]\n\u001b[0;32m--> 976\u001b[0m     normalizer \u001b[39m=\u001b[39m proba\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m    977\u001b[0m     normalizer[normalizer \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m    978\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m normalizer\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mode = 'classification_rf' #sys.argv[1]\n",
    "\n",
    "if mode == \"classification_rf\":\n",
    "    datasets = [\"internet\"]\n",
    "#     datasets = [\"adult\", \"amazon\", \"click\", \"internet\", \n",
    "#                 \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "                \n",
    "    create_dir(\"results/models\")\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        # Training all models\n",
    "        print(\"training models...\")\n",
    "        generate_rf_ensemble_classification(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1ae06e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = credit\n",
      "training models...\n"
     ]
    }
   ],
   "source": [
    "mode = 'classification_rf' #sys.argv[1]\n",
    "\n",
    "if mode == \"classification_rf\":\n",
    "    datasets = [\"credit\"]\n",
    "#     datasets = [\"adult\", \"amazon\", \"click\", \"internet\", \n",
    "#                 \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "                \n",
    "    create_dir(\"results/models\")\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        # Training all models\n",
    "        print(\"training models...\")\n",
    "        generate_rf_ensemble_classification(name)\n",
    "        ## This function is generate_rf_ensemble_classification training models, this is the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3611071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d45f70",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c6d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from catboost.utils import read_cd\n",
    "from gbdt_uncertainty.assessment import prr_class, ood_detect, nll_class\n",
    "from gbdt_uncertainty.uncertainty import entropy_of_expected_class, expected_entropy_class, entropy\n",
    "from sklearn.metrics import zero_one_loss, log_loss\n",
    "from scipy.stats import ttest_rel\n",
    "import math\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e85d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['sgb-fixed', 'sglb-fixed'] \n",
    "\n",
    "# for proper tables\n",
    "convert_name = {\"adult\": \"Adult\", \"amazon\": \"Amazon\", \"click\": \"Click\", \n",
    "                \"internet\": \"Internet\", \"appetency\": \"KDD-Appetency\", \"churn\": \"KDD-Churn\",\n",
    "                \"upselling\": \"KDD-Upselling\", \"kick\": \"Kick\", 'credit': 'Credit-Card'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e17cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "     \n",
    "def load_model(name, alg, i):\n",
    "    if alg == \"rf\":\n",
    "        model = joblib.load(\"results/models/\" + name + \"_\" + alg + \"_\" + str(i))\n",
    "    else:\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(\"results/models/\" + name + \"_\" + alg + \"_\" + str(i)) \n",
    "    return model\n",
    "    \n",
    "def rf_virtual_ensembles_predict(model, X, count=10):\n",
    "    trees = model.estimators_\n",
    "    num_trees = len(trees)\n",
    "    ens_preds = []\n",
    "    for i in range(count):\n",
    "        indices = range(int(i*num_trees/count), int((i+1)*num_trees/count))\n",
    "        all_preds = []\n",
    "        for ind in indices:\n",
    "            all_preds.append(trees[ind].predict_proba(X))\n",
    "        all_preds = np.array(all_preds)\n",
    "        preds = np.mean(all_preds, axis=0)\n",
    "        ens_preds.append(preds)\n",
    "    ens_preds = np.array(ens_preds)\n",
    "\n",
    "    return np.swapaxes(ens_preds, 0, 1)\n",
    "    \n",
    "def virtual_ensembles_predict(X, model, alg, num_models=10):\n",
    "    if alg == \"rf\":\n",
    "        all_preds = rf_virtual_ensembles_predict(model, X, count=num_models)\n",
    "    else:\n",
    "        all_preds = model.virtual_ensembles_predict(X, prediction_type='VirtEnsembles', virtual_ensembles_count=num_models)\n",
    "        all_preds = sigmoid(all_preds)\n",
    "        all_preds = np.concatenate((1 - all_preds, all_preds), axis=2)\n",
    "    return np.swapaxes(all_preds, 0, 1)\n",
    "    \n",
    "def compute_significance(values_all, metric, minimize=True):\n",
    "\n",
    "    values_mean = np.mean(values_all, axis=1) \n",
    "    \n",
    "    # choose best algorithm\n",
    "    if minimize:\n",
    "        best_idx = np.nanargmin(values_mean)\n",
    "    else:\n",
    "        best_idx = np.nanargmax(values_mean)\n",
    "        \n",
    "    textbf = {best_idx} # for all algorithms insignificantly different from the best one\n",
    "    # compute statistical significance on test\n",
    "\n",
    "    for idx in range(len(values_mean)):\n",
    "        test = ttest_rel(values_all[best_idx], values_all[idx]) # paired t-test\n",
    "        if test[1] > 0.05:\n",
    "            textbf.add(idx)\n",
    "            \n",
    "    return values_mean, textbf\n",
    "\n",
    "def compute_best(values, minimize=True):\n",
    "\n",
    "    # choose best algorithm\n",
    "    if minimize:\n",
    "        best_idx = np.nanargmin(values)\n",
    "    else:\n",
    "        best_idx = np.nanargmax(values)\n",
    "        \n",
    "    textbf = {best_idx} \n",
    "    for idx in range(len(values)):\n",
    "        if values[best_idx] == values[idx]: \n",
    "            textbf.add(idx)\n",
    "            \n",
    "    return textbf\n",
    "    \n",
    "def make_table_entry(values_all, metric, minimize=True, round=2):\n",
    "    \n",
    "    num_values = len(values_all)\n",
    "    \n",
    "    values_mean, textbf = compute_significance(values_all, metric, minimize=minimize)\n",
    "\n",
    "    # prepare all results in latex format\n",
    "\n",
    "    table = \"\"\n",
    "\n",
    "    for idx in range(num_values):\n",
    "        if idx in textbf:\n",
    "            table += \"\\\\textbf{\" + str(np.round(values_mean[idx], round)) + \"} \"\n",
    "        else:    \n",
    "            table += str(np.round(values_mean[idx], round)) + \" \"\n",
    "        table += \"& \" \n",
    "            \n",
    "    return table\n",
    "\n",
    "def normalize_test_labels(y_test):\n",
    "    y_test_norm = []\n",
    "    c0 = min(y_test)\n",
    "    for y in y_test:\n",
    "        if y == c0:\n",
    "            y_test_norm.append(0)\n",
    "        else:\n",
    "            y_test_norm.append(1)\n",
    "    return np.array(y_test_norm)\n",
    "            \n",
    "def aggregate_results(name, modes = [\"single\", \"ens\", \"virt\"], \n",
    "                      algorithms = ['sgb-fixed', 'sglb-fixed'], num_models = 10):\n",
    "    \n",
    "\n",
    "    results = [] # metric values for all algorithms and all folds\n",
    "        \n",
    "    for mode in modes:\n",
    "        for alg in algorithms:\n",
    "        \n",
    "            if alg == \"rf\":\n",
    "                train_pool, y_train, test_pool, y_test, enc = process_classification_dataset(name)\n",
    "                \n",
    "                # process ood data\n",
    "                cd = read_cd(\"datasets/\"+name+\"/pool.cd\", data_file = \"datasets/\"+name+\"/test\")\n",
    "                try: \n",
    "                    label_ind = cd['column_type_to_indices']['Label']\n",
    "                except:\n",
    "                    label_ind = cd['column_type_to_indices']['Target']\n",
    "\n",
    "                ood_test_pool = np.loadtxt(\"datasets/ood/\" + name + \"_rf\", delimiter=\"\\t\", dtype=\"object\")\n",
    "                ood_test_pool = enc.transform(ood_test_pool).astype(\"float64\")\n",
    "                ood_test_pool = np.delete(ood_test_pool, label_ind, 1)\n",
    "                ood_size = len(ood_test_pool)\n",
    "                \n",
    "            else:\n",
    "                test_pool = Pool(data=\"datasets/\"+name+\"/test\", column_description=\"datasets/\"+name+\"/pool.cd\")\n",
    "                ood_test_pool = Pool(data=\"datasets/ood/\" + name, column_description=\"datasets/\"+name+\"/pool.cd\")\n",
    "                ood_size = ood_test_pool.num_row()\n",
    "\n",
    "                y_test = test_pool.get_label()\n",
    "            \n",
    "            test_size = len(y_test)\n",
    "            domain_labels = np.concatenate([np.zeros(test_size), np.ones(ood_size)])\n",
    "                    \n",
    "            y_test_norm = normalize_test_labels(y_test)\n",
    "        \n",
    "            values = defaultdict() # metric values for all folds for given algorithm\n",
    "\n",
    "            if mode == \"single\":\n",
    "                # use 0th model from ensemble as a single model\n",
    "                model = load_model(name, alg, 0)\n",
    "                preds = model.predict(test_pool)\n",
    "                preds_proba = model.predict_proba(test_pool)\n",
    "    \n",
    "                values[\"error\"] = (preds != y_test).astype(int)\n",
    "                values[\"nll\"] = nll_class(y_test_norm, preds_proba)\n",
    "                values[\"TU_prr\"] = prr_class(y_test_norm, preds_proba, entropy(preds_proba), False)\n",
    "                values[\"KU_prr\"] = float(\"nan\")\n",
    "                values[\"KU_auc\"] = float(\"nan\")\n",
    "                    \n",
    "                ood_preds_proba = model.predict_proba(ood_test_pool)\n",
    "                in_measure = entropy(preds_proba)\n",
    "                out_measure = entropy(ood_preds_proba)\n",
    "                values[\"TU_auc\"] = ood_detect(domain_labels, in_measure, out_measure, mode=\"ROC\")\n",
    "\n",
    "            if mode == \"ens\":\n",
    "                all_preds = [] # predictions of all models in ensemble\n",
    "                all_preds_ood = []\n",
    "                    \n",
    "                for i in range(num_models):\n",
    "                    model = load_model(name, alg, i)\n",
    "                    preds = model.predict_proba(test_pool)\n",
    "                    all_preds.append(preds)\n",
    "                    preds = model.predict_proba(ood_test_pool)\n",
    "                    all_preds_ood.append(preds) \n",
    "                        \n",
    "                all_preds = np.array(all_preds)\n",
    "                preds_proba = np.mean(all_preds, axis=0)\n",
    "                \n",
    "                all_preds_ood = np.array(all_preds_ood)\n",
    "                \n",
    "                preds = np.argmax(preds_proba, axis=1)\n",
    "                values[\"error\"] = (preds != y_test_norm).astype(int)\n",
    "                values[\"nll\"] = nll_class(y_test_norm, preds_proba)\n",
    "                \n",
    "                TU = entropy_of_expected_class(all_preds)\n",
    "                DU = expected_entropy_class(all_preds)\n",
    "                KU = TU - DU\n",
    "                \n",
    "                TU_ood = entropy_of_expected_class(all_preds_ood)\n",
    "                DU_ood = expected_entropy_class(all_preds_ood)\n",
    "                KU_ood = TU_ood - DU_ood\n",
    "\n",
    "                values[\"TU_prr\"] = prr_class(y_test_norm, preds_proba, TU, False)\n",
    "                values[\"KU_prr\"] = prr_class(y_test_norm, preds_proba, KU, False)\n",
    "                  \n",
    "                values[\"TU_auc\"] = ood_detect(domain_labels, TU, TU_ood, mode=\"ROC\")\n",
    "                values[\"KU_auc\"] = ood_detect(domain_labels, KU, KU_ood, mode=\"ROC\")\n",
    "                        \n",
    "            if mode == \"virt\":\n",
    "                if alg in [\"sgb\", \"sgb-fixed\"]: # we do not evaluate virtual sgb model\n",
    "                    continue\n",
    "                    \n",
    "                # generate virtual ensemble from 0th model\n",
    "                model = load_model(name, alg, 0)\n",
    "\n",
    "                all_preds = virtual_ensembles_predict(test_pool, model, alg)\n",
    "                \n",
    "                preds_proba = np.mean(all_preds, axis=0)\n",
    "    \n",
    "                preds = np.argmax(preds_proba, axis=1)\n",
    "                values[\"error\"] = (preds != y_test_norm).astype(int)\n",
    "                values[\"nll\"] = nll_class(y_test_norm, preds_proba)\n",
    "                \n",
    "                TU = entropy_of_expected_class(all_preds)\n",
    "                DU = expected_entropy_class(all_preds)\n",
    "                KU = TU - DU\n",
    "                \n",
    "                all_preds_ood = virtual_ensembles_predict(ood_test_pool, model, alg)\n",
    "                TU_ood = entropy_of_expected_class(all_preds_ood)\n",
    "                DU_ood = expected_entropy_class(all_preds_ood)\n",
    "                KU_ood = TU_ood - DU_ood\n",
    "\n",
    "                values[\"TU_prr\"] = prr_class(y_test_norm, preds_proba, TU, False)\n",
    "                values[\"KU_prr\"] = prr_class(y_test_norm, preds_proba, KU, False)\n",
    "                  \n",
    "                values[\"TU_auc\"] = ood_detect(domain_labels, TU, TU_ood, mode=\"ROC\")\n",
    "                values[\"KU_auc\"] = ood_detect(domain_labels, KU, KU_ood, mode=\"ROC\")\n",
    "                        \n",
    "            if mode == \"virt\" and alg in [\"sgb\", \"sgb-fixed\"]: # we do not evaluate virtual sgb model\n",
    "                continue\n",
    "            \n",
    "            results.append(values)\n",
    "\n",
    "    return np.array(results)\n",
    "    \n",
    "def make_table_element(mean, textbf, idx):\n",
    "    table = \"\"\n",
    "    if np.isnan(mean[idx]):\n",
    "        table += \"--- & \"\n",
    "        return table\n",
    "    if idx in textbf:\n",
    "        table += \"\\\\textbf{\" + str(int(np.rint(mean[idx]))) + \"} \"\n",
    "    else:    \n",
    "        table += str(int(np.rint(mean[idx]))) + \" \"\n",
    "    table += \"& \"\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d31eca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Comparison with random forest, PRR and AUC-ROC===\n",
      "\\multirow{2}{*} {Internet} & TU & 79 & 69 & \\textbf{79} & 68 & 64 & 75 & 64 & 74 \\\\\n",
      " & KU & 57 & 38 & 72 & 36 & 98 & 92 & \\textbf{100} & 92 \\\\\n",
      "\\midrule\n"
     ]
    }
   ],
   "source": [
    "table_type = 'rf_prr_auc' #sys.argv[1]\n",
    "\n",
    "datasets = [\"internet\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "if table_type == \"rf_prr_auc\":\n",
    "    print(\"===Comparison with random forest, PRR and AUC-ROC===\")\n",
    "        \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, algorithms=[\"sglb-fixed\", \"rf\"], modes=[\"virt\", \"ens\"])\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        textbf_prr = compute_best(prr, minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        textbf_auc = compute_best(auc, minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU & \"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832c2599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Comparison with random forest, NLL and Error===\n",
      "Internet & \\textbf{0.217} & 0.275 & \\textbf{0.217} & 0.274 & \\textbf{10.0} & 11.2 & \\textbf{10.0} & 11.0 \\\\\n"
     ]
    }
   ],
   "source": [
    "table_type = 'rf_nll_error' #sys.argv[1]\n",
    "\n",
    "datasets = [\"internet\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "if table_type == \"rf_nll_error\": \n",
    "\n",
    "    print(\"===Comparison with random forest, NLL and Error===\")\n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, algorithms=[\"sglb-fixed\", \"rf\"], modes=[\"single\", \"ens\"])\n",
    "        \n",
    "        table = convert_name[name] + \" & \"\n",
    "        \n",
    "        values_nll = np.array([values[i][\"nll\"] for i in range(len(values))])\n",
    "        values_error = np.array([values[i][\"error\"] for i in range(len(values))])\n",
    "        \n",
    "        table += make_table_entry(values_nll, \"nll\", round=3)\n",
    "        table += make_table_entry(values_error*100, \"error\", round=1)\n",
    "        \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1df3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41de8e83",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a5311e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Comparison with random forest, PRR and AUC-ROC===\n",
      "\\multirow{2}{*} {Credit-Card} & TU & 45 & \\textbf{55} & 46 & 55 & 78 & 77 & 75 & 76 \\\\\n",
      " & KU & 11 & 37 & 18 & 39 & 92 & 55 & \\textbf{99} & 60 \\\\\n",
      "\\midrule\n"
     ]
    }
   ],
   "source": [
    "table_type = 'rf_prr_auc' #sys.argv[1]\n",
    "\n",
    "datasets = [\"credit\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "if table_type == \"rf_prr_auc\":\n",
    "    print(\"===Comparison with random forest, PRR and AUC-ROC===\")\n",
    "        \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, algorithms=[\"sglb-fixed\", \"rf\"], modes=[\"virt\", \"ens\"])\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        textbf_prr = compute_best(prr, minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        textbf_auc = compute_best(auc, minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU & \"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba96dda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Comparison with random forest, NLL and Error===\n",
      "Credit-Card & \\textbf{0.405} & \\textbf{0.409} & \\textbf{0.405} & \\textbf{0.409} & \\textbf{16.7} & 17.2 & \\textbf{16.7} & 17.2 \\\\\n"
     ]
    }
   ],
   "source": [
    "table_type = 'rf_nll_error' #sys.argv[1]\n",
    "\n",
    "datasets = [\"credit\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "if table_type == \"rf_nll_error\": \n",
    "\n",
    "    print(\"===Comparison with random forest, NLL and Error===\")\n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, algorithms=[\"sglb-fixed\", \"rf\"], modes=[\"single\", \"ens\"])\n",
    "        \n",
    "        table = convert_name[name] + \" & \"\n",
    "        \n",
    "        values_nll = np.array([values[i][\"nll\"] for i in range(len(values))])\n",
    "        values_error = np.array([values[i][\"error\"] for i in range(len(values))])\n",
    "        \n",
    "        table += make_table_entry(values_nll, \"nll\", round=3)\n",
    "        table += make_table_entry(values_error*100, \"error\", round=1)\n",
    "        \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac491684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cfdead377f2f078dff013c5b6eb4c6432bd9ee60c1303d07d941effa38613f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
