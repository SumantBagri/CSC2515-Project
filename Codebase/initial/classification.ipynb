{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b160238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from catboost import Pool, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"classification\" #sys.argv[1]\n",
    "\n",
    "def create_dir(name):\n",
    "    directory = os.path.dirname(name)\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb17f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_parameters_classification(dataset_name, alg='sgb'):\n",
    "\n",
    "    # load and prepare data\n",
    "    data_dir = os.path.join('datasets', dataset_name)\n",
    "    train_file = os.path.join(data_dir, 'train')\n",
    "    validation_file = os.path.join(data_dir, 'validation')\n",
    "    test_file = os.path.join(data_dir, 'test')\n",
    "    cd_file = os.path.join(data_dir, 'pool.cd')\n",
    "    \n",
    "    train_pool = Pool(data=train_file, column_description=cd_file)\n",
    "    validation_pool = Pool(data=validation_file, column_description=cd_file)\n",
    "    test_pool = Pool(data=test_file, column_description=cd_file)\n",
    "    \n",
    "\n",
    "    seed = 1000 # starting random seed for hyperparameter tuning\n",
    "    \n",
    "    # list of hyperparameters for grid search\n",
    "    # we do not tune the number of trees, it is important for virtual ensembles\n",
    "    depths = [3, 4, 5, 6] # tree depth\n",
    "    lrs = [0.001, 0.01, 0.1] # learning rate \n",
    "    if alg == \"sgb\" or alg == \"sglb\": # by default, we tune sample rate\n",
    "        samples = [0.25, 0.5, 0.75]\n",
    "    if alg == \"sgb-fixed\": # sgb without sample rate tuning\n",
    "        samples = [0.5]\n",
    "    if alg == \"sglb-fixed\": # sglb without sample rate tuning\n",
    "        samples = [1.0]\n",
    "    shape = (len(depths), len(lrs), len(samples))\n",
    "\n",
    "    # perform grid search\n",
    "    results = np.zeros(shape)\n",
    "    for d, depth in enumerate(depths):\n",
    "        for l, lr in enumerate(lrs):\n",
    "            for s, sample in enumerate(samples):\n",
    "                if alg == 'sgb' or alg == 'sgb-fixed':\n",
    "                    model = CatBoostClassifier(loss_function='Logloss',learning_rate=lr, depth=depth, subsample=sample, bootstrap_type='Bernoulli', verbose=False, random_seed=seed)                      \n",
    "                if alg == 'sglb' or alg == 'sglb-fixed':\n",
    "                    model = CatBoostClassifier(loss_function='Logloss',learning_rate=lr, depth=depth, subsample=sample, bootstrap_type='Bernoulli', verbose=False, random_seed=seed, posterior_sampling=True)\n",
    "                    \n",
    "                model.fit(train_pool, eval_set=validation_pool, use_best_model=False)\n",
    "                    \n",
    "                # compute nll\n",
    "                results[d, l, s] = model.evals_result_['validation']['Logloss'][-1]\n",
    "                    \n",
    "                seed += 1 # update seed\n",
    "        \n",
    "    # get best parameters\n",
    "    argmin = np.unravel_index(np.argmin(results), shape)\n",
    "    depth = depths[argmin[0]]\n",
    "    lr = lrs[argmin[1]]\n",
    "    sample = samples[argmin[2]]\n",
    "        \n",
    "    params = {'depth': depth, 'lr': lr, 'sample': sample}\n",
    "    \n",
    "    return params\n",
    "    \n",
    "def generate_ensemble_classification(dataset_name, params, alg=\"sgb\", num_models=10):\n",
    "\n",
    "    # load and prepare data\n",
    "    data_dir = os.path.join('datasets', dataset_name)\n",
    "    full_train_file = os.path.join(data_dir, 'full_train')\n",
    "    test_file = os.path.join(data_dir, 'test')\n",
    "    cd_file = os.path.join(data_dir, 'pool.cd')\n",
    "    \n",
    "    full_train_pool = Pool(data=full_train_file, column_description=cd_file)\n",
    "    test_pool = Pool(data=test_file, column_description=cd_file)\n",
    "\n",
    "    # parameters\n",
    "    depth = params['depth']\n",
    "    lr = params['lr']\n",
    "    sample = params['sample']\n",
    "        \n",
    "    seed = 0\n",
    "    for i in range(num_models):\n",
    "        if alg == 'sgb' or alg == 'sgb-fixed':\n",
    "            model = CatBoostClassifier(loss_function='Logloss', verbose=False, \n",
    "                                       learning_rate=lr, depth=depth, subsample=sample,\n",
    "                                       bootstrap_type='Bernoulli', custom_metric='ZeroOneLoss', \n",
    "                                       random_seed=seed)   \n",
    "        if alg == 'sglb' or alg == 'sglb-fixed':\n",
    "            model = CatBoostClassifier(loss_function='Logloss', verbose=False, \n",
    "                                       learning_rate=lr, depth=depth, subsample=sample, \n",
    "                                       bootstrap_type='Bernoulli', posterior_sampling=True, \n",
    "                                       custom_metric='ZeroOneLoss', random_seed=seed)\n",
    "        seed += 1 # new seed for each ensemble element\n",
    "\n",
    "        model.fit(full_train_pool, eval_set=test_pool, use_best_model=False) # do not use test pool for choosing best iteration\n",
    "        model.save_model(\"results/models/\" + dataset_name + \"_\" + alg + \"_\" + str(i), format=\"cbm\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6331e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = churn\n",
      "training models...\n",
      "sgb-fixed\n",
      "sglb-fixed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == \"classification\":\n",
    "\n",
    "    tuning = 0 #int(sys.argv[2])\n",
    "    \n",
    "    datasets = [\"churn\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "    algorithms = ['sgb-fixed', 'sglb-fixed'] # choose from ['sgb-fixed', 'sglb-fixed', 'sgb', 'sglb'] \n",
    "    # for -fixed we do not tune sample rate and use 0.5 for sbf and 1. for sglb\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        if tuning == 1:\n",
    "            create_dir(\"results/params\")\n",
    "        \n",
    "            # Tune hyperparameters\n",
    "            print(\"tuning hyperparameters...\")\n",
    "            for alg in algorithms:\n",
    "                print(alg)\n",
    "                params = tune_parameters_classification(name, alg=alg)\n",
    "                with open(\"results/params/\" + name + \"_\" + alg + '.json', 'w') as fp:\n",
    "                    json.dump(params, fp)\n",
    "                    \n",
    "        # Training all models\n",
    "        print(\"training models...\")\n",
    "        create_dir(\"results/models\")\n",
    "        for alg in algorithms:\n",
    "            print(alg)\n",
    "            with open(\"results/params/\" + name + \"_\" + alg + '.json', 'r') as fp:\n",
    "                params = json.load(fp)\n",
    "            generate_ensemble_classification(name, params, alg=alg)\n",
    "        print()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3709daf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = credit\n",
      "training models...\n",
      "sgb-fixed\n",
      "sglb-fixed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == \"classification\":\n",
    "\n",
    "    tuning = 0 #int(sys.argv[2])\n",
    "    \n",
    "    datasets = [\"credit\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "    algorithms = ['sgb-fixed', 'sglb-fixed'] # choose from ['sgb-fixed', 'sglb-fixed', 'sgb', 'sglb'] \n",
    "    # for -fixed we do not tune sample rate and use 0.5 for sbf and 1. for sglb\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        if tuning == 1:\n",
    "            create_dir(\"results/params\")\n",
    "        \n",
    "            # Tune hyperparameters\n",
    "            print(\"tuning hyperparameters...\")\n",
    "            for alg in algorithms:\n",
    "                print(alg)\n",
    "                params = tune_parameters_classification(name, alg=alg)\n",
    "                with open(\"results/params/\" + name + \"_\" + alg + '.json', 'w') as fp:\n",
    "                    json.dump(params, fp)\n",
    "                    \n",
    "        # Training all models\n",
    "        print(\"training models...\")\n",
    "        create_dir(\"results/models\")\n",
    "        for alg in algorithms:\n",
    "            print(alg)\n",
    "            with open(\"results/params/\" + name + \"_\" + alg + '.json', 'r') as fp:\n",
    "                params = json.load(fp)\n",
    "            generate_ensemble_classification(name, params, alg=alg)\n",
    "        print()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d27921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91621477",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d005f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from catboost.utils import read_cd\n",
    "from gbdt_uncertainty.data import process_classification_dataset\n",
    "from gbdt_uncertainty.assessment import prr_class, ood_detect, nll_class\n",
    "from gbdt_uncertainty.uncertainty import entropy_of_expected_class, expected_entropy_class, entropy\n",
    "from sklearn.metrics import zero_one_loss, log_loss\n",
    "from scipy.stats import ttest_rel\n",
    "import math\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf4c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['sgb-fixed', 'sglb-fixed'] \n",
    "\n",
    "# for proper tables\n",
    "convert_name = {\"adult\": \"Adult\", \"amazon\": \"Amazon\", \"click\": \"Click\", \n",
    "                \"internet\": \"Internet\", \"appetency\": \"KDD-Appetency\", \"churn\": \"KDD-Churn\",\n",
    "                \"upselling\": \"KDD-Upselling\", \"kick\": \"Kick\", 'credit': 'Credit-Card'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32778043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "     \n",
    "def load_model(name, alg, i):\n",
    "    if alg == \"rf\":\n",
    "        model = joblib.load(\"results/models/\" + name + \"_\" + alg + \"_\" + str(i))\n",
    "    else:\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(\"results/models/\" + name + \"_\" + alg + \"_\" + str(i)) \n",
    "    return model\n",
    "    \n",
    "def rf_virtual_ensembles_predict(model, X, count=10):\n",
    "    trees = model.estimators_\n",
    "    num_trees = len(trees)\n",
    "    ens_preds = []\n",
    "    for i in range(count):\n",
    "        indices = range(int(i*num_trees/count), int((i+1)*num_trees/count))\n",
    "        all_preds = []\n",
    "        for ind in indices:\n",
    "            all_preds.append(trees[ind].predict_proba(X))\n",
    "        all_preds = np.array(all_preds)\n",
    "        preds = np.mean(all_preds, axis=0)\n",
    "        ens_preds.append(preds)\n",
    "    ens_preds = np.array(ens_preds)\n",
    "\n",
    "    return np.swapaxes(ens_preds, 0, 1)\n",
    "    \n",
    "def virtual_ensembles_predict(X, model, alg, num_models=10):\n",
    "    if alg == \"rf\":\n",
    "        all_preds = rf_virtual_ensembles_predict(model, X, count=num_models)\n",
    "    else:\n",
    "        all_preds = model.virtual_ensembles_predict(X, prediction_type='VirtEnsembles', virtual_ensembles_count=num_models)\n",
    "        all_preds = sigmoid(all_preds)\n",
    "        all_preds = np.concatenate((1 - all_preds, all_preds), axis=2)\n",
    "    return np.swapaxes(all_preds, 0, 1)\n",
    "    \n",
    "def compute_significance(values_all, metric, minimize=True):\n",
    "\n",
    "    values_mean = np.mean(values_all, axis=1) \n",
    "    \n",
    "    # choose best algorithm\n",
    "    if minimize:\n",
    "        best_idx = np.nanargmin(values_mean)\n",
    "    else:\n",
    "        best_idx = np.nanargmax(values_mean)\n",
    "        \n",
    "    textbf = {best_idx} # for all algorithms insignificantly different from the best one\n",
    "    # compute statistical significance on test\n",
    "\n",
    "    for idx in range(len(values_mean)):\n",
    "        test = ttest_rel(values_all[best_idx], values_all[idx]) # paired t-test\n",
    "        if test[1] > 0.05:\n",
    "            textbf.add(idx)\n",
    "            \n",
    "    return values_mean, textbf\n",
    "\n",
    "def compute_best(values, minimize=True):\n",
    "\n",
    "    # choose best algorithm\n",
    "    if minimize:\n",
    "        best_idx = np.nanargmin(values)\n",
    "    else:\n",
    "        best_idx = np.nanargmax(values)\n",
    "        \n",
    "    textbf = {best_idx} \n",
    "    for idx in range(len(values)):\n",
    "        if values[best_idx] == values[idx]: \n",
    "            textbf.add(idx)\n",
    "            \n",
    "    return textbf\n",
    "    \n",
    "def make_table_entry(values_all, metric, minimize=True, round=2):\n",
    "    \n",
    "    num_values = len(values_all)\n",
    "    \n",
    "    values_mean, textbf = compute_significance(values_all, metric, minimize=minimize)\n",
    "\n",
    "    # prepare all results in latex format\n",
    "\n",
    "    table = \"\"\n",
    "\n",
    "    for idx in range(num_values):\n",
    "        if idx in textbf:\n",
    "            table += \"\\\\textbf{\" + str(np.round(values_mean[idx], round)) + \"} \"\n",
    "        else:    \n",
    "            table += str(np.round(values_mean[idx], round)) + \" \"\n",
    "        table += \"& \" \n",
    "            \n",
    "    return table\n",
    "\n",
    "def normalize_test_labels(y_test):\n",
    "    y_test_norm = []\n",
    "    c0 = min(y_test)\n",
    "    for y in y_test:\n",
    "        if y == c0:\n",
    "            y_test_norm.append(0)\n",
    "        else:\n",
    "            y_test_norm.append(1)\n",
    "    return np.array(y_test_norm)\n",
    "            \n",
    "def aggregate_results(name, modes = [\"single\", \"ens\", \"virt\"], \n",
    "                      algorithms = ['sgb-fixed', 'sglb-fixed'], num_models = 10):\n",
    "    \n",
    "\n",
    "    results = [] # metric values for all algorithms and all folds\n",
    "        \n",
    "    for mode in modes:\n",
    "        for alg in algorithms:\n",
    "        \n",
    "            if alg == \"rf\":\n",
    "                train_pool, y_train, test_pool, y_test, enc = process_classification_dataset(name)\n",
    "                \n",
    "                # process ood data\n",
    "                cd = read_cd(\"datasets/\"+name+\"/pool.cd\", data_file = \"datasets/\"+name+\"/test\")\n",
    "                try: \n",
    "                    label_ind = cd['column_type_to_indices']['Label']\n",
    "                except:\n",
    "                    label_ind = cd['column_type_to_indices']['Target']\n",
    "\n",
    "                ood_test_pool = np.loadtxt(\"datasets/ood/\" + name, delimiter=\"\\t\", dtype=\"object\")\n",
    "                ood_test_pool = enc.transform(ood_test_pool).astype(\"float64\")\n",
    "                ood_test_pool = np.delete(ood_test_pool, label_ind, 1)\n",
    "                ood_size = len(ood_test_pool)\n",
    "                \n",
    "            else:\n",
    "                test_pool = Pool(data=\"datasets/\"+name+\"/test\", column_description=\"datasets/\"+name+\"/pool.cd\")\n",
    "                ood_test_pool = Pool(data=\"datasets/ood/\" + name, column_description=\"datasets/\"+name+\"/pool.cd\")\n",
    "                ood_size = ood_test_pool.num_row()\n",
    "\n",
    "                y_test = test_pool.get_label()\n",
    "            \n",
    "            test_size = len(y_test)\n",
    "            domain_labels = np.concatenate([np.zeros(test_size), np.ones(ood_size)])\n",
    "                    \n",
    "            y_test_norm = normalize_test_labels(y_test)\n",
    "        \n",
    "            values = defaultdict() # metric values for all folds for given algorithm\n",
    "\n",
    "            if mode == \"single\":\n",
    "                # use 0th model from ensemble as a single model\n",
    "                model = load_model(name, alg, 0)\n",
    "                preds = model.predict(test_pool)\n",
    "                preds_proba = model.predict_proba(test_pool)\n",
    "    \n",
    "                values[\"error\"] = (preds != y_test).astype(int)\n",
    "                values[\"nll\"] = nll_class(y_test_norm, preds_proba)\n",
    "                values[\"TU_prr\"] = prr_class(y_test_norm, preds_proba, entropy(preds_proba), False)\n",
    "                values[\"KU_prr\"] = float(\"nan\")\n",
    "                values[\"KU_auc\"] = float(\"nan\")\n",
    "                    \n",
    "                ood_preds_proba = model.predict_proba(ood_test_pool)\n",
    "                in_measure = entropy(preds_proba)\n",
    "                out_measure = entropy(ood_preds_proba)\n",
    "                values[\"TU_auc\"] = ood_detect(domain_labels, in_measure, out_measure, mode=\"ROC\")\n",
    "\n",
    "            if mode == \"ens\":\n",
    "                all_preds = [] # predictions of all models in ensemble\n",
    "                all_preds_ood = []\n",
    "                    \n",
    "                for i in range(num_models):\n",
    "                    model = load_model(name, alg, i)\n",
    "                    preds = model.predict_proba(test_pool)\n",
    "                    all_preds.append(preds)\n",
    "                    preds = model.predict_proba(ood_test_pool)\n",
    "                    all_preds_ood.append(preds) \n",
    "                        \n",
    "                all_preds = np.array(all_preds)\n",
    "                preds_proba = np.mean(all_preds, axis=0)\n",
    "                \n",
    "                all_preds_ood = np.array(all_preds_ood)\n",
    "                \n",
    "                preds = np.argmax(preds_proba, axis=1)\n",
    "                values[\"error\"] = (preds != y_test_norm).astype(int)\n",
    "                values[\"nll\"] = nll_class(y_test_norm, preds_proba)\n",
    "                \n",
    "                TU = entropy_of_expected_class(all_preds)\n",
    "                DU = expected_entropy_class(all_preds)\n",
    "                KU = TU - DU\n",
    "                \n",
    "                TU_ood = entropy_of_expected_class(all_preds_ood)\n",
    "                DU_ood = expected_entropy_class(all_preds_ood)\n",
    "                KU_ood = TU_ood - DU_ood\n",
    "\n",
    "                values[\"TU_prr\"] = prr_class(y_test_norm, preds_proba, TU, False)\n",
    "                values[\"KU_prr\"] = prr_class(y_test_norm, preds_proba, KU, False)\n",
    "                  \n",
    "                values[\"TU_auc\"] = ood_detect(domain_labels, TU, TU_ood, mode=\"ROC\")\n",
    "                values[\"KU_auc\"] = ood_detect(domain_labels, KU, KU_ood, mode=\"ROC\")\n",
    "                        \n",
    "            if mode == \"virt\":\n",
    "                if alg in [\"sgb\", \"sgb-fixed\"]: # we do not evaluate virtual sgb model\n",
    "                    continue\n",
    "                    \n",
    "                # generate virtual ensemble from 0th model\n",
    "                model = load_model(name, alg, 0)\n",
    "\n",
    "                all_preds = virtual_ensembles_predict(test_pool, model, alg)\n",
    "                \n",
    "                preds_proba = np.mean(all_preds, axis=0)\n",
    "    \n",
    "                preds = np.argmax(preds_proba, axis=1)\n",
    "                values[\"error\"] = (preds != y_test_norm).astype(int)\n",
    "                values[\"nll\"] = nll_class(y_test_norm, preds_proba)\n",
    "                \n",
    "                TU = entropy_of_expected_class(all_preds)\n",
    "                DU = expected_entropy_class(all_preds)\n",
    "                KU = TU - DU\n",
    "                \n",
    "                all_preds_ood = virtual_ensembles_predict(ood_test_pool, model, alg)\n",
    "                TU_ood = entropy_of_expected_class(all_preds_ood)\n",
    "                DU_ood = expected_entropy_class(all_preds_ood)\n",
    "                KU_ood = TU_ood - DU_ood\n",
    "\n",
    "                values[\"TU_prr\"] = prr_class(y_test_norm, preds_proba, TU, False)\n",
    "                values[\"KU_prr\"] = prr_class(y_test_norm, preds_proba, KU, False)\n",
    "                  \n",
    "                values[\"TU_auc\"] = ood_detect(domain_labels, TU, TU_ood, mode=\"ROC\")\n",
    "                values[\"KU_auc\"] = ood_detect(domain_labels, KU, KU_ood, mode=\"ROC\")\n",
    "                        \n",
    "            if mode == \"virt\" and alg in [\"sgb\", \"sgb-fixed\"]: # we do not evaluate virtual sgb model\n",
    "                continue\n",
    "            \n",
    "            results.append(values)\n",
    "\n",
    "    return np.array(results)\n",
    "    \n",
    "def make_table_element(mean, textbf, idx):\n",
    "    table = \"\"\n",
    "    if np.isnan(mean[idx]):\n",
    "        table += \"--- & \"\n",
    "        return table\n",
    "    if idx in textbf:\n",
    "        table += \"\\\\textbf{\" + str(int(np.rint(mean[idx]))) + \"} \"\n",
    "    else:    \n",
    "        table += str(int(np.rint(mean[idx]))) + \" \"\n",
    "    table += \"& \"\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ad361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===PRR and AUC-ROC Table===\n",
      "\\multirow{2}{*} {Internet} & TU & 75 & 79 & 78 & \\textbf{79} & 79 & \\textbf{50} & \\textbf{50} & \\textbf{50} & \\textbf{50} & \\textbf{50} \\\\\n",
      " & KU & --- & --- & 72 & 72 & 57 & --- & --- & \\textbf{50} & \\textbf{50} & \\textbf{50} \\\\\n",
      "\\midrule\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"internet\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "table_type = \"prr_auc\" #sys.argv[1]\n",
    "\n",
    "if table_type == \"prr_auc\":\n",
    "    print(\"===PRR and AUC-ROC Table===\")\n",
    "    \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name)\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        textbf_prr = compute_best(prr, minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        \n",
    "        textbf_auc = compute_best(auc, minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU & \"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82496b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===PRR and AUC-ROC Table===\n",
      "\\multirow{2}{*} {Credit-Card} & TU & 45 & \\textbf{46} & 46 & 46 & 45 & 78 & 75 & 80 & 75 & 78 \\\\\n",
      " & KU & --- & --- & 20 & 18 & 11 & --- & --- & 99 & \\textbf{99} & 92 \\\\\n",
      "\\midrule\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"credit\"] #[\"adult\", \"amazon\", \"click\", \"internet\", \"appetency\", \"churn\", \"upselling\", \"kick\"]\n",
    "\n",
    "table_type = \"prr_auc\" #sys.argv[1]\n",
    "\n",
    "if table_type == \"prr_auc\":\n",
    "    print(\"===PRR and AUC-ROC Table===\")\n",
    "    \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name)\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        textbf_prr = compute_best(prr, minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        \n",
    "        textbf_auc = compute_best(auc, minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU & \"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db675000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
