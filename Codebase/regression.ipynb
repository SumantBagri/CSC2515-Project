{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff32f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from gbdt_uncertainty.training import generate_ensemble_regression\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32310cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regression\" #sys.argv[1]\n",
    "\n",
    "def create_dir(name):\n",
    "    directory = os.path.dirname(name)\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b44dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_regression_dataset(name):\n",
    "    if name == \"YearPredictionMSD\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD\n",
    "        data = np.loadtxt(\"datasets/\" + name + \".txt\", delimiter=\",\")\n",
    "        n_splits = 1\n",
    "        index_features = [i for i in range(1, 91)]\n",
    "        index_target = 0\n",
    "    else:\n",
    "        # repository with all UCI datasets\n",
    "        url = \"https://raw.githubusercontent.com/yaringal/DropoutUncertaintyExps/master/UCI_Datasets/\" + name + \"/data/\"\n",
    "        data = np.loadtxt(url + \"data.txt\")\n",
    "        n_splits = int(np.loadtxt(url + \"n_splits.txt\"))\n",
    "        index_features = [int(i) for i in np.loadtxt(url + \"index_features.txt\")]\n",
    "        index_target = int(np.loadtxt(url + \"index_target.txt\"))\n",
    "\n",
    "    X = data[:, index_features]  # features\n",
    "    y = data[:, index_target]  # target\n",
    "\n",
    "    # prepare data for all train/test splits\n",
    "    index_train = []\n",
    "    index_test = []\n",
    "    for i in range(n_splits):\n",
    "        if name == \"YearPredictionMSD\":\n",
    "            # default split for this dataset, see https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD\n",
    "            index_train.append([i for i in range(463715)])\n",
    "            index_test.append([i for i in range(463715, 515345)])\n",
    "        else:\n",
    "            index_train.append([int(i) for i in np.loadtxt(url + \"index_train_\" + str(i) + \".txt\")])\n",
    "            index_test.append([int(i) for i in np.loadtxt(url + \"index_test_\" + str(i) + \".txt\")])\n",
    "\n",
    "    return X, y, index_train, index_test, n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3132b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_test(X, y, index_train, index_test, fold):\n",
    "    # train_all consists of all train instances\n",
    "    X_train_all = X[index_train[fold], :]\n",
    "    y_train_all = y[index_train[fold]]\n",
    "\n",
    "    X_test = X[index_test[fold], :]\n",
    "    y_test = y[index_test[fold]]\n",
    "\n",
    "    # for parameter tuning we use 20% of train dataset for validation\n",
    "    num_training_examples = int(0.8 * X_train_all.shape[0])\n",
    "    X_train = X_train_all[0:num_training_examples, :]\n",
    "    y_train = y_train_all[0:num_training_examples]\n",
    "    X_validation = X_train_all[num_training_examples:, :]\n",
    "    y_validation = y_train_all[num_training_examples:]\n",
    "\n",
    "    return X_train_all, y_train_all, X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5883164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_parameters_regression(X, y, index_train, index_test, n_splits, alg='sgb'):\n",
    "\n",
    "    params = []\n",
    "    seed = 1000 # starting random seed for hyperparameter tuning\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "\n",
    "        # make catboost pools\n",
    "        X_train_all, y_train_all, X_train, y_train, X_validation, y_validation, X_test, y_test = make_train_val_test(X, y, index_train, index_test, fold)\n",
    "        full_train_pool = Pool(X_train_all, y_train_all)\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        validation_pool = Pool(X_validation, y_validation)\n",
    "        test_pool = Pool(X_test, y_test)\n",
    "        \n",
    "        # list of hyperparameters for grid search\n",
    "        # we do not tune the number of trees, it is important for virtual ensembles\n",
    "        depths = [3, 4, 5, 6] # tree depth\n",
    "        lrs = [0.001, 0.01, 0.1] # learning rate \n",
    "        if alg == \"sgb\" or alg == \"sglb\": # by default, we tune sample rate\n",
    "            samples = [0.25, 0.5, 0.75]\n",
    "        if alg == \"sgb-fixed\": # sgb without sample rate tuning\n",
    "            samples = [0.5]\n",
    "        if alg == \"sglb-fixed\": # sglb without sample rate tuning\n",
    "            samples = [1.0]\n",
    "        shape = (len(depths), len(lrs), len(samples))\n",
    "\n",
    "        # perform grid search\n",
    "        results = np.zeros(shape)\n",
    "        for d, depth in enumerate(depths):\n",
    "            for l, lr in enumerate(lrs):\n",
    "                for s, sample in enumerate(samples):\n",
    "                    if alg == 'sgb' or alg == 'sgb-fixed':\n",
    "                        model = CatBoostRegressor(loss_function='RMSEWithUncertainty',\n",
    "                                                  learning_rate=lr, depth=depth, \n",
    "                                                  subsample=sample, bootstrap_type='Bernoulli', verbose=False, \n",
    "                                                  random_seed=seed)                      \n",
    "                    if alg == 'sglb' or alg == 'sglb-fixed':\n",
    "                        model = CatBoostRegressor(loss_function='RMSEWithUncertainty',\n",
    "                                                  learning_rate=lr, depth=depth, \n",
    "                                                  subsample=sample, \n",
    "                                                  bootstrap_type='Bernoulli', \n",
    "                                                  verbose=False, random_seed=seed, posterior_sampling=True,\n",
    "                                                 allow_writing_files=False)\n",
    "                    \n",
    "                    model.fit(train_pool, eval_set=validation_pool, use_best_model=False)\n",
    "                    \n",
    "                    # compute nll\n",
    "                    results[d, l, s] = model.evals_result_['validation']['RMSEWithUncertainty'][-1]\n",
    "                    \n",
    "                    seed += 1 # update seed\n",
    "        \n",
    "        # get best parameters\n",
    "        argmin = np.unravel_index(np.argmin(results), shape)\n",
    "        depth = depths[argmin[0]]\n",
    "        lr = lrs[argmin[1]]\n",
    "        sample = samples[argmin[2]]\n",
    "        \n",
    "        current_params = {'depth': depth, 'lr': lr, 'sample': sample}\n",
    "        params.append(current_params)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02479683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_parameters_regression_parkinsons(dataset_name, alg='sgb', n_splits=1):\n",
    "\n",
    "    # load and prepare data\n",
    "    data_dir = os.path.join('datasets', dataset_name)\n",
    "    train_file = os.path.join(data_dir, 'train')\n",
    "    validation_file = os.path.join(data_dir, 'validation')\n",
    "    test_file = os.path.join(data_dir, 'test')\n",
    "    cd_file = os.path.join(data_dir, 'pool.cd')\n",
    "    \n",
    "    train_pool = Pool(data=train_file, column_description=cd_file)\n",
    "    validation_pool = Pool(data=validation_file, column_description=cd_file)\n",
    "    test_pool = Pool(data=test_file, column_description=cd_file)\n",
    "    \n",
    "\n",
    "    params = []\n",
    "    seed = 1000 # starting random seed for hyperparameter tuning\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "    \n",
    "        # list of hyperparameters for grid search\n",
    "        # we do not tune the number of trees, it is important for virtual ensembles\n",
    "        depths = [3, 4, 5, 6] # tree depth\n",
    "        lrs = [0.001, 0.01, 0.1] # learning rate \n",
    "        if alg == \"sgb\" or alg == \"sglb\": # by default, we tune sample rate\n",
    "            samples = [0.25, 0.5, 0.75]\n",
    "        if alg == \"sgb-fixed\": # sgb without sample rate tuning\n",
    "            samples = [0.5]\n",
    "        if alg == \"sglb-fixed\": # sglb without sample rate tuning\n",
    "            samples = [1.0]\n",
    "        shape = (len(depths), len(lrs), len(samples))\n",
    "\n",
    "        # perform grid search\n",
    "        results = np.zeros(shape)\n",
    "        for d, depth in enumerate(depths):\n",
    "            for l, lr in enumerate(lrs):\n",
    "                for s, sample in enumerate(samples):\n",
    "                    if alg == 'sgb' or alg == 'sgb-fixed':\n",
    "                            model = CatBoostRegressor(loss_function='RMSEWithUncertainty',\n",
    "                                                      learning_rate=lr, depth=depth, \n",
    "                                                      subsample=sample, bootstrap_type='Bernoulli', verbose=False, \n",
    "                                                      random_seed=seed)                      \n",
    "                    if alg == 'sglb' or alg == 'sglb-fixed':\n",
    "                        model = CatBoostRegressor(loss_function='RMSEWithUncertainty',\n",
    "                                                  learning_rate=lr, depth=depth, \n",
    "                                                  subsample=sample, \n",
    "                                                  bootstrap_type='Bernoulli', \n",
    "                                                  verbose=False, random_seed=seed, posterior_sampling=True,\n",
    "                                                 allow_writing_files=False)\n",
    "\n",
    "                    model.fit(train_pool, eval_set=validation_pool, use_best_model=False)\n",
    "\n",
    "                # compute nll\n",
    "                results[d, l, s] = model.evals_result_['validation']['RMSEWithUncertainty'][-1]\n",
    "\n",
    "                seed += 1 # update seed\n",
    "\n",
    "        # get best parameters\n",
    "        argmin = np.unravel_index(np.argmin(results), shape)\n",
    "        depth = depths[argmin[0]]\n",
    "        lr = lrs[argmin[1]]\n",
    "        sample = samples[argmin[2]]\n",
    "\n",
    "        current_params = {'depth': depth, 'lr': lr, 'sample': sample}\n",
    "        params.append(current_params)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b378238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_regression(dataset_name, X, y, index_train, index_test, n_splits, params, alg=\"sgb\", num_models=10):\n",
    "\n",
    "    for fold in range(n_splits):\n",
    "\n",
    "        # make catboost pools\n",
    "        X_train_all, y_train_all, X_train, y_train, X_validation, y_validation, X_test, y_test = make_train_val_test(X, y, index_train, index_test, fold)\n",
    "        full_train_pool = Pool(X_train_all, y_train_all)\n",
    "        test_pool = Pool(X_test, y_test)\n",
    "    \n",
    "        # params contains optimal parameters for each fold\n",
    "        depth = params[fold]['depth']\n",
    "        lr = params[fold]['lr']\n",
    "        sample = params[fold]['sample']\n",
    "\n",
    "        seed = 10 * fold # fix different starting random seeds for all folds\n",
    "        for i in range(num_models):\n",
    "            if alg == 'sgb' or alg == 'sgb-fixed':\n",
    "                model = CatBoostRegressor(loss_function='RMSEWithUncertainty', verbose=False, \n",
    "                                          learning_rate=lr, depth=depth, subsample=sample,\n",
    "                                          bootstrap_type='Bernoulli', custom_metric='RMSE', \n",
    "                                          random_seed=seed)   \n",
    "            if alg == 'sglb' or alg == 'sglb-fixed':\n",
    "                model = CatBoostRegressor(loss_function='RMSEWithUncertainty', verbose=False, \n",
    "                                          learning_rate=lr, depth=depth, subsample=sample, \n",
    "                                          bootstrap_type='Bernoulli', posterior_sampling=True, \n",
    "                                          custom_metric='RMSE', random_seed=seed, \n",
    "                                         allow_writing_files=False) #, task_type=\"GPU\", devices='0')\n",
    "            seed += 1 # new seed for each ensemble element\n",
    "\n",
    "            model.fit(full_train_pool, eval_set=test_pool, use_best_model=False) # do not use test pool for choosing best iteration\n",
    "            model.save_model(\"results/models/\" + dataset_name + \"_\" + alg + \"_f\" + str(fold) + \"_\" + str(i), format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a23ca6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_regression_parkinsions(dataset_name, params, alg=\"sgb\", num_models=10, n_splits=1):\n",
    "\n",
    "    for fold in range(n_splits):\n",
    "        \n",
    "        # load and prepare data\n",
    "        data_dir = os.path.join('datasets', dataset_name)\n",
    "        full_train_file = os.path.join(data_dir, 'full_train')\n",
    "        test_file = os.path.join(data_dir, 'test')\n",
    "        cd_file = os.path.join(data_dir, 'pool.cd')\n",
    "\n",
    "        full_train_pool = Pool(data=full_train_file, column_description=cd_file)\n",
    "        test_pool = Pool(data=test_file, column_description=cd_file)\n",
    "\n",
    "        # params contains optimal parameters for each fold\n",
    "        depth = params[fold]['depth']\n",
    "        lr = params[fold]['lr']\n",
    "        sample = params[fold]['sample']\n",
    "\n",
    "        seed = 10 * fold # fix different starting random seeds for all folds\n",
    "        for i in range(num_models):\n",
    "            if alg == 'sgb' or alg == 'sgb-fixed':\n",
    "                    model = CatBoostRegressor(loss_function='RMSEWithUncertainty', verbose=False, \n",
    "                                              learning_rate=lr, depth=depth, subsample=sample,\n",
    "                                              bootstrap_type='Bernoulli', custom_metric='RMSE', \n",
    "                                              random_seed=seed)   \n",
    "            if alg == 'sglb' or alg == 'sglb-fixed':\n",
    "                model = CatBoostRegressor(loss_function='RMSEWithUncertainty', verbose=False, \n",
    "                                          learning_rate=lr, depth=depth, subsample=sample, \n",
    "                                          bootstrap_type='Bernoulli', posterior_sampling=True, \n",
    "                                          custom_metric='RMSE', random_seed=seed, \n",
    "                                         allow_writing_files=False) #, task_type=\"GPU\", devices='0')\n",
    "            seed += 1 # new seed for each ensemble element        \n",
    "\n",
    "            model.fit(full_train_pool, eval_set=test_pool, use_best_model=False) # do not use test pool for choosing best iteration\n",
    "            model.save_model(\"results/models/\" + dataset_name + \"_\" + alg + \"_f\" + str(fold) + \"_\" + str(i), format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e482c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = bostonHousing\n",
      "training models...\n",
      "sgb-fixed\n",
      "sglb-fixed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == \"regression\":\n",
    "\n",
    "    try:\n",
    "        tuning = 0 #int(sys.argv[2])\n",
    "    except:\n",
    "        print(\"Tuning parameter is required: 1 if tuning is needed\")\n",
    "        exit(0)\n",
    "    \n",
    "    datasets = [\"bostonHousing\"]\n",
    "#     datasets = [\"bostonHousing\", \"concrete\", \"energy\", \"kin8nm\", \n",
    "#                 \"naval-propulsion-plant\", \"power-plant\", \"protein-tertiary-structure\",\n",
    "#                 \"wine-quality-red\", \"yacht\", \"YearPredictionMSD\"]    \n",
    "\n",
    "    algorithms = ['sgb-fixed', 'sglb-fixed'] \n",
    "    # for -fixed we do not tune sample rate and use 0.5 for sbf and 1. for sglb\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        if tuning == 1:\n",
    "            create_dir(\"results/params\")\n",
    "        \n",
    "            # Tune hyperparameters\n",
    "            print(\"tuning hyperparameters...\")\n",
    "            \n",
    "            X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "            for alg in algorithms:\n",
    "                print(alg)\n",
    "                params = tune_parameters_regression(X, y, index_train, \n",
    "                                                    index_test, n_splits, alg=alg)\n",
    "                with open(\"results/params/\" + name + \"_\" + alg + '.json', 'w') as fp:\n",
    "                    json.dump(params, fp)\n",
    "            \n",
    "        # Training models\n",
    "        print(\"training models...\")\n",
    "        create_dir(\"results/models\")\n",
    "        \n",
    "        for alg in algorithms:\n",
    "            print(alg)\n",
    "            X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "            with open(\"results/params/\" + name + \"_\" + alg + '.json', 'r') as fp:\n",
    "                params = json.load(fp)\n",
    "            generate_ensemble_regression(name, X, y, index_train, index_test, \n",
    "                                         n_splits, params, alg=alg)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8840028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = yacht\n",
      "tuning hyperparameters...\n",
      "sgb-fixed\n",
      "sglb-fixed\n",
      "training models...\n",
      "sgb-fixed\n",
      "sglb-fixed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == \"regression\":\n",
    "\n",
    "    try:\n",
    "        tuning = 1 #int(sys.argv[2])\n",
    "    except:\n",
    "        print(\"Tuning parameter is required: 1 if tuning is needed\")\n",
    "        exit(0)\n",
    "    \n",
    "    datasets = [\"yacht\"]\n",
    "#     datasets = [\"bostonHousing\", \"concrete\", \"energy\", \"kin8nm\", \n",
    "#                 \"naval-propulsion-plant\", \"power-plant\", \"protein-tertiary-structure\",\n",
    "#                 \"wine-quality-red\", \"yacht\", \"YearPredictionMSD\"]    \n",
    "\n",
    "    algorithms = ['sgb-fixed', 'sglb-fixed'] \n",
    "    # for -fixed we do not tune sample rate and use 0.5 for sbf and 1. for sglb\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        if tuning == 1:\n",
    "            create_dir(\"results/params\")\n",
    "        \n",
    "            # Tune hyperparameters\n",
    "            print(\"tuning hyperparameters...\")\n",
    "            \n",
    "            X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "            for alg in algorithms:\n",
    "                print(alg)\n",
    "                params = tune_parameters_regression(X, y, index_train, \n",
    "                                                    index_test, n_splits, alg=alg)\n",
    "                with open(\"results/params/\" + name + \"_\" + alg + '.json', 'w') as fp:\n",
    "                    json.dump(params, fp)\n",
    "            \n",
    "        # Training models\n",
    "        print(\"training models...\")\n",
    "        create_dir(\"results/models\")\n",
    "        \n",
    "        for alg in algorithms:\n",
    "            print(alg)\n",
    "            X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "            with open(\"results/params/\" + name + \"_\" + alg + '.json', 'r') as fp:\n",
    "                params = json.load(fp)\n",
    "            generate_ensemble_regression(name, X, y, index_train, index_test, \n",
    "                                         n_splits, params, alg=alg)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "782966f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = parkinsons\n",
      "training models...\n",
      "sgb-fixed\n",
      "sglb-fixed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == \"regression\":\n",
    "\n",
    "    try:\n",
    "        tuning = 0 #int(sys.argv[2])\n",
    "    except:\n",
    "        print(\"Tuning parameter is required: 1 if tuning is needed\")\n",
    "        exit(0)\n",
    "    \n",
    "    datasets = [\"parkinsons\"]\n",
    "#     datasets = [\"bostonHousing\", \"concrete\", \"energy\", \"kin8nm\", \n",
    "#                 \"naval-propulsion-plant\", \"power-plant\", \"protein-tertiary-structure\",\n",
    "#                 \"wine-quality-red\", \"yacht\", \"YearPredictionMSD\"]    \n",
    "\n",
    "    algorithms = ['sgb-fixed', 'sglb-fixed'] \n",
    "    # for -fixed we do not tune sample rate and use 0.5 for sbf and 1. for sglb\n",
    "    \n",
    "    for name in datasets:\n",
    "        print(\"dataset =\", name)\n",
    "    \n",
    "        if tuning == 1:\n",
    "            create_dir(\"results/params\")\n",
    "        \n",
    "            # Tune hyperparameters\n",
    "            print(\"tuning hyperparameters...\")\n",
    "            \n",
    "#             X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "            for alg in algorithms:\n",
    "                print(alg)\n",
    "                params = tune_parameters_regression_parkinsons(name, alg=alg)\n",
    "                with open(\"results/params/\" + name + \"_\" + alg + '.json', 'w') as fp:\n",
    "                    json.dump(params, fp)\n",
    "            \n",
    "        # Training models\n",
    "        print(\"training models...\")\n",
    "        create_dir(\"results/models\")\n",
    "        \n",
    "        for alg in algorithms:\n",
    "            print(alg)\n",
    "#             X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "            with open(\"results/params/\" + name + \"_\" + alg + '.json', 'r') as fp:\n",
    "                params = json.load(fp)\n",
    "            generate_ensemble_regression_parkinsions(name, params, alg=alg)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7df916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784e9622",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fb94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from scipy.stats import ttest_rel\n",
    "from gbdt_uncertainty.assessment import prr_regression, nll_regression, ens_nll_regression, ood_detect\n",
    "from gbdt_uncertainty.uncertainty import ensemble_uncertainties_regression\n",
    "import math\n",
    "import joblib\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07af7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bostonHousing\"]\n",
    "# datasets = [\"bostonHousing\", \"concrete\", \"energy\", \"kin8nm\", \"naval-propulsion-plant\",\n",
    "#             \"power-plant\", \"protein-tertiary-structure\", \"wine-quality-red\", \"yacht\", \n",
    "#             \"YearPredictionMSD\"]\n",
    "algorithms = ['sgb-fixed', 'sglb-fixed'] \n",
    "\n",
    "# for proper tables\n",
    "convert_name = {\"bostonHousing\": \"BostonH\", \"yacht\": \"Yacht\", 'parkinsons': 'Parkinsons'}\n",
    "# convert_name = {\"bostonHousing\": \"BostonH\", \"concrete\": \"Concrete\", \"energy\": \"Energy\", \n",
    "#                 \"kin8nm\": \"Kin8nm\", \"naval-propulsion-plant\": \"Naval-p\", \"power-plant\": \"Power-p\",\n",
    "#                 \"protein-tertiary-structure\": \"Protein\", \"wine-quality-red\": \"Wine-qu\", \n",
    "#                 \"yacht\": \"Yacht\", \"YearPredictionMSD\": \"Year\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f0a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rmse(preds, target, raw=False):\n",
    "    if raw:\n",
    "        return (preds - target)**2 # for individual predictions\n",
    "    return np.sqrt(np.mean((preds - target)**2))\n",
    "\n",
    "def ens_rmse(target, preds, epsilon=1e-8, raw=False):\n",
    "    means = preds[:, :, 0] \n",
    "    avg_mean = np.mean(means, axis=0) \n",
    "    if raw: # for individual predictions\n",
    "        return calc_rmse(avg_mean, target, raw=True)\n",
    "    return calc_rmse(avg_mean, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafbe167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict(X, name, alg, fold, i):\n",
    "    if alg == \"rf\":\n",
    "        model = joblib.load(\"results/models/\" + name + \"_\" + alg + \"_f\" + str(fold) + \"_\" + str(i))\n",
    "        preds = model.predict(X)\n",
    "        preds = np.array([(p, 1) for p in preds]) # 1 for unknown variance\n",
    "    else:\n",
    "        model = CatBoostRegressor()\n",
    "        model.load_model(\"results/models/\" + name + \"_\" + alg + \"_f\" + str(fold) + \"_\" + str(i)) \n",
    "        preds = model.predict(X)\n",
    "    return preds, model\n",
    "    \n",
    "def predict(X, model, alg):\n",
    "    preds = model.predict(X)\n",
    "    if alg == \"rf\":\n",
    "        preds = np.array([(p, 1) for p in preds])\n",
    "    return preds\n",
    "    \n",
    "def rf_virtual_ensembles_predict(model, X, count=10):\n",
    "    trees = model.estimators_\n",
    "    num_trees = len(trees)\n",
    "    ens_preds = []\n",
    "    for i in range(count):\n",
    "        indices = range(int(i*num_trees/count), int((i+1)*num_trees/count))\n",
    "        all_preds = []\n",
    "        for ind in indices:\n",
    "            all_preds.append(trees[ind].predict(X))\n",
    "        all_preds = np.array(all_preds)\n",
    "        preds = np.mean(all_preds, axis=0)\n",
    "        preds = np.array([(p, 1) for p in preds]) # 1 for unknown variance\n",
    "        ens_preds.append(preds)\n",
    "    ens_preds = np.array(ens_preds)\n",
    "\n",
    "    return np.swapaxes(ens_preds, 0, 1)\n",
    "    \n",
    "def virtual_ensembles_load_and_predict(X, name, alg, fold, i, num_models=10):\n",
    "    if alg == \"rf\":\n",
    "        model = joblib.load(\"results/models/\" + name + \"_\" + alg + \"_f\" + str(fold) + \"_\" + str(i))\n",
    "        all_preds = rf_virtual_ensembles_predict(model, X)\n",
    "    else:\n",
    "        model = CatBoostRegressor()\n",
    "        model.load_model(\"results/models/\" + name + \"_\" + alg + \"_f\" + str(fold) + \"_\" + str(i)) \n",
    "        all_preds = model.virtual_ensembles_predict(X, prediction_type='VirtEnsembles', virtual_ensembles_count=num_models)\n",
    "    return np.swapaxes(all_preds, 0, 1), model\n",
    "  \n",
    "def virtual_ensembles_predict(X, model, alg, num_models=10):\n",
    "    if alg == \"rf\":\n",
    "        all_preds = rf_virtual_ensembles_predict(model, X)\n",
    "    else:\n",
    "        all_preds = model.virtual_ensembles_predict(X, prediction_type='VirtEnsembles', virtual_ensembles_count=num_models)\n",
    "    return np.swapaxes(all_preds, 0, 1)\n",
    "    \n",
    "def compute_significance(values_all, metric, minimize=True, raw=False):\n",
    "\n",
    "    if raw:\n",
    "        values_all = values_all[:, 0, :]\n",
    "\n",
    "    values_mean = np.mean(values_all, axis=1) # mean wrt folds or elements\n",
    "    \n",
    "    if raw and metric == \"rmse\":\n",
    "        values_mean = np.sqrt(values_mean)\n",
    "    \n",
    "    # choose best algorithm\n",
    "    if minimize:\n",
    "        best_idx = np.nanargmin(values_mean)\n",
    "    else:\n",
    "        best_idx = np.nanargmax(values_mean)\n",
    "        \n",
    "    textbf = {best_idx} # for all algorithms insignificantly different from the best one\n",
    "    # compute statistical significance on test or wrt folds\n",
    "\n",
    "    for idx in range(len(values_mean)):\n",
    "        test = ttest_rel(values_all[best_idx], values_all[idx]) # paired t-test\n",
    "        if test[1] > 0.05:\n",
    "            textbf.add(idx)\n",
    "            \n",
    "    return values_mean, textbf\n",
    "\n",
    "def compute_best(values, minimize=True):\n",
    "\n",
    "    # choose best algorithm\n",
    "    if minimize:\n",
    "        best_idx = np.nanargmin(values)\n",
    "    else:\n",
    "        best_idx = np.nanargmax(values)\n",
    "        \n",
    "    textbf = {best_idx} \n",
    "    for idx in range(len(values)):\n",
    "        if values[best_idx] == values[idx]: \n",
    "            textbf.add(idx)\n",
    "            \n",
    "    return textbf\n",
    "    \n",
    "def make_table_entry(values_all, metric, minimize=True, round=2, raw=True):\n",
    "    \n",
    "    num_values = len(values_all)\n",
    "    \n",
    "    values_mean, textbf = compute_significance(values_all, metric, minimize=minimize, raw=raw)\n",
    "\n",
    "    # prepare all results in latex format\n",
    "\n",
    "    table = \"\"\n",
    "\n",
    "    for idx in range(num_values):\n",
    "        if idx in textbf:\n",
    "            table += \"\\\\textbf{\" + str(np.round(values_mean[idx], round)) + \"} \"\n",
    "        else:    \n",
    "            table += str(np.round(values_mean[idx], round)) + \" \"\n",
    "        table += \"& \" \n",
    "            \n",
    "    return table\n",
    "            \n",
    "def aggregate_results(name, modes = [\"single\", \"ens\", \"virt\"], \n",
    "                      algorithms = ['sgb-fixed', 'sglb-fixed'], num_models = 10, \n",
    "                      raw=False):\n",
    "    \n",
    "    n_splits=1\n",
    "    if name != 'parkinsons':\n",
    "        X, y, index_train, index_test, n_splits = load_regression_dataset(name)\n",
    "    \n",
    "    results = [] # metric values for all algorithms and all folds\n",
    "    \n",
    "    # for ood evaluation\n",
    "    ood_X_test = np.loadtxt(\"datasets/ood/\" + name)\n",
    "    if name == \"naval-propulsion-plant\":\n",
    "        ood_X_test = ood_X_test[:, :-1]\n",
    "    ood_size = len(ood_X_test)\n",
    "        \n",
    "    for mode in modes:\n",
    "        for alg in algorithms:\n",
    "        \n",
    "            values = defaultdict(lambda: []) # metric values for all folds for given algorithm\n",
    "\n",
    "            for fold in range(n_splits):\n",
    "                if name != 'parkinsons':\n",
    "                    X_train_all, y_train_all, X_train, y_train, X_validation, y_validation, X_test, y_test = make_train_val_test(\n",
    "                                                                                        X, y, index_train, index_test, fold)\n",
    "                else:\n",
    "                    ood_test_pool = Pool(data=\"datasets/ood/\" + name, column_description=\"datasets/\"+name+\"/pool.cd\")\n",
    "                    X_test, y_test = ood_test_pool.get_features(), ood_test_pool.get_label()\n",
    "                    y_test = np.array(y_test).astype(np.float64)\n",
    "                test_size = len(X_test)\n",
    "                domain_labels = np.concatenate([np.zeros(test_size), np.ones(ood_size)])\n",
    "\n",
    "                if mode == \"single\":\n",
    "                    # use 0th model from ensemble as a single model\n",
    "                    preds, model = load_and_predict(X_test, name, alg, fold, 0)\n",
    "\n",
    "                    values[\"rmse\"].append(calc_rmse(preds[:, 0], y_test, raw=raw))\n",
    "                    values[\"nll\"].append(nll_regression(y_test, preds[:, 0], preds[:, 1], raw=raw))\n",
    "                    values[\"TU_prr\"].append(prr_regression(y_test, preds[:, 0], preds[:, 1]))\n",
    "                    values[\"KU_prr\"].append(float(\"nan\"))\n",
    "                    values[\"KU_auc\"].append(float(\"nan\"))\n",
    "                    \n",
    "                    ood_preds = predict(ood_X_test, model, alg)\n",
    "                    in_measure = preds[:, 1]\n",
    "                    out_measure = ood_preds[:, 1]\n",
    "                    values[\"TU_auc\"].append(ood_detect(domain_labels, in_measure, out_measure, mode=\"ROC\"))\n",
    "\n",
    "                if mode == \"ens\":\n",
    "                    all_preds = [] # predictions of all models in ensemble\n",
    "                    all_preds_ood = []\n",
    "                    \n",
    "                    for i in range(num_models):\n",
    "                        preds, model = load_and_predict(X_test, name, alg, fold, i)\n",
    "                        all_preds.append(preds)\n",
    "                        preds = predict(ood_X_test, model, alg)\n",
    "                        all_preds_ood.append(preds)   \n",
    "                    all_preds = np.array(all_preds)\n",
    "                    \n",
    "                    values[\"rmse\"].append(ens_rmse(y_test, all_preds, raw=raw))\n",
    "                    values[\"nll\"].append(ens_nll_regression(y_test, all_preds, raw=raw)) \n",
    "                    \n",
    "                    TU = ensemble_uncertainties_regression(np.swapaxes(all_preds, 0, 1))[\"tvar\"]\n",
    "                    KU = ensemble_uncertainties_regression(np.swapaxes(all_preds, 0, 1))[\"varm\"]\n",
    "\n",
    "                    mean_preds = np.mean(all_preds[:, :, 0], axis=0)\n",
    "\n",
    "                    values[\"TU_prr\"].append(prr_regression(y_test, mean_preds, TU))\n",
    "                    values[\"KU_prr\"].append(prr_regression(y_test, mean_preds, KU))\n",
    "                    \n",
    "                    all_preds_ood = np.array(all_preds_ood)\n",
    "                    TU_ood = ensemble_uncertainties_regression(np.swapaxes(all_preds_ood, 0, 1))[\"tvar\"]\n",
    "                    KU_ood = ensemble_uncertainties_regression(np.swapaxes(all_preds_ood, 0, 1))[\"varm\"]\n",
    "                    values[\"TU_auc\"].append(ood_detect(domain_labels, TU, TU_ood, mode=\"ROC\"))\n",
    "                    values[\"KU_auc\"].append(ood_detect(domain_labels, KU, KU_ood, mode=\"ROC\"))\n",
    "                        \n",
    "                if mode == \"virt\":\n",
    "                    if alg in [\"sgb\", \"sgb-fixed\"]: # we do not evaluate virtual sgb model\n",
    "                        continue\n",
    "                    # generate virtual ensemble from 0th model\n",
    "                    all_preds, model = virtual_ensembles_load_and_predict(X_test, name, alg, fold, 0)\n",
    "\n",
    "                    values[\"rmse\"].append(ens_rmse(y_test, all_preds, raw=raw))\n",
    "                    values[\"nll\"].append(ens_nll_regression(y_test, all_preds, raw=raw)) \n",
    "                    \n",
    "                    TU = ensemble_uncertainties_regression(np.swapaxes(all_preds, 0, 1))[\"tvar\"]\n",
    "                    KU = ensemble_uncertainties_regression(np.swapaxes(all_preds, 0, 1))[\"varm\"]\n",
    "                    \n",
    "                    mean_preds = np.mean(all_preds[:, :, 0], axis=0)\n",
    "\n",
    "                    values[\"TU_prr\"].append(prr_regression(y_test, mean_preds, TU))\n",
    "                    values[\"KU_prr\"].append(prr_regression(y_test, mean_preds, KU))\n",
    "                    \n",
    "                    all_preds_ood = virtual_ensembles_predict(ood_X_test, model, alg)\n",
    "                    all_preds_ood = np.array(all_preds_ood)\n",
    "                    \n",
    "                    TU_ood = ensemble_uncertainties_regression(np.swapaxes(all_preds_ood, 0, 1))[\"tvar\"]\n",
    "                    KU_ood = ensemble_uncertainties_regression(np.swapaxes(all_preds_ood, 0, 1))[\"varm\"]\n",
    "                    \n",
    "                    values[\"TU_auc\"].append(ood_detect(domain_labels, TU, TU_ood, mode=\"ROC\"))\n",
    "                    values[\"KU_auc\"].append(ood_detect(domain_labels, KU, KU_ood, mode=\"ROC\"))\n",
    "\n",
    "            if mode == \"virt\" and alg in [\"sgb\", \"sgb-fixed\"]: # we do not evaluate virtual sgb model\n",
    "                continue\n",
    "            \n",
    "            results.append(values)\n",
    "\n",
    "    return np.array(results)\n",
    "    \n",
    "def make_table_element(mean, textbf, idx):\n",
    "    table = \"\"\n",
    "    if np.isnan(mean[idx]):\n",
    "        table += \"--- & \"\n",
    "        return table\n",
    "    if idx in textbf:\n",
    "        table += \"\\\\textbf{\" + str(int(np.rint(mean[idx]))) + \"} \"\n",
    "    else:    \n",
    "        table += str(int(np.rint(mean[idx]))) + \" \"\n",
    "    table += \"& \"\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a052ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===PRR and AUC-ROC Table===\n",
      "\\multirow{2}{*} {BostonH} & TU &\\textbf{47} & \\textbf{42} & \\textbf{47} & \\textbf{44} & \\textbf{44} & 96 & 95 & 96 & 95 & 96 \\\\\n",
      " & KU & --- & --- & \\textbf{39} & \\textbf{41} & \\textbf{38} & --- & --- & \\textbf{99} & \\textbf{98} & \\textbf{94} \\\\\n",
      "\\midrule\n"
     ]
    }
   ],
   "source": [
    "table_type = \"prr_auc\" #sys.argv[1]\n",
    "\n",
    "if table_type == \"prr_auc\":\n",
    "    print(\"===PRR and AUC-ROC Table===\")\n",
    "    \n",
    "    datasets = [\"bostonHousing\"]\n",
    "        \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, raw=False)\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = 100*np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        mean_prr, textbf_prr = compute_significance(prr, \"prr\", minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        mean_auc, textbf_auc = compute_significance(auc, \"auc\", minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU &\"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(mean_prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(mean_auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(mean_prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(mean_auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22ced16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===PRR and AUC-ROC Table===\n",
      "\\multirow{2}{*} {Yacht} & TU &\\textbf{88} & \\textbf{87} & \\textbf{88} & \\textbf{88} & \\textbf{87} & 77 & 70 & 79 & 74 & 81 \\\\\n",
      " & KU & --- & --- & 74 & 81 & 68 & --- & --- & \\textbf{87} & \\textbf{83} & \\textbf{84} \\\\\n",
      "\\midrule\n"
     ]
    }
   ],
   "source": [
    "table_type = \"prr_auc\" #sys.argv[1]\n",
    "\n",
    "if table_type == \"prr_auc\":\n",
    "    print(\"===PRR and AUC-ROC Table===\")\n",
    "    \n",
    "    datasets = [\"yacht\"]\n",
    "        \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, raw=False)\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = 100*np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        mean_prr, textbf_prr = compute_significance(prr, \"prr\", minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        mean_auc, textbf_auc = compute_significance(auc, \"auc\", minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU &\"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(mean_prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(mean_auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(mean_prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(mean_auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2f1284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===PRR and AUC-ROC Table===\n",
      "\\multirow{2}{*} {Parkinsons} & TU &-15 & -50 & 6 & -15 & -36 & \\textbf{100} & 97 & 88 & 64 & 93 \\\\\n",
      " & KU & --- & --- & \\textbf{11} & -13 & -46 & --- & --- & 85 & 59 & 80 \\\\\n",
      "\\midrule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajes\\AppData\\Local\\Temp\\ipykernel_3644\\3199549680.py:72: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  test = ttest_rel(values_all[best_idx], values_all[idx]) # paired t-test\n",
      "d:\\onedrive - university of toronto\\csc2515 - intro to ml\\project\\gbdt-uncertainty-main\\csc2515_venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1253: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "d:\\onedrive - university of toronto\\csc2515 - intro to ml\\project\\gbdt-uncertainty-main\\csc2515_venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n"
     ]
    }
   ],
   "source": [
    "table_type = \"prr_auc\" #sys.argv[1]\n",
    "\n",
    "if table_type == \"prr_auc\":\n",
    "    print(\"===PRR and AUC-ROC Table===\")\n",
    "    \n",
    "    datasets = [\"parkinsons\"]\n",
    "        \n",
    "    for name in datasets:\n",
    "\n",
    "        values = aggregate_results(name, raw=False)\n",
    "        \n",
    "        prr_TU = np.array([values[i][\"TU_prr\"] for i in range(len(values))])\n",
    "        prr_KU = np.array([values[i][\"KU_prr\"] for i in range(len(values))])\n",
    "        prr = 100*np.concatenate((prr_TU, prr_KU), axis=0)\n",
    "\n",
    "        mean_prr, textbf_prr = compute_significance(prr, \"prr\", minimize=False)\n",
    "    \n",
    "        auc_TU = np.array([values[i][\"TU_auc\"] for i in range(len(values))])\n",
    "        auc_KU = np.array([values[i][\"KU_auc\"] for i in range(len(values))])\n",
    "        auc = 100*np.concatenate((auc_TU, auc_KU), axis=0)\n",
    "        mean_auc, textbf_auc = compute_significance(auc, \"auc\", minimize=False)\n",
    "\n",
    "        num = len(auc_TU)\n",
    "    \n",
    "        table = \"\\multirow{2}{*} {\" + convert_name[name] + \"} & TU &\"\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(mean_prr, textbf_prr, idx)\n",
    "\n",
    "        for idx in range(num):\n",
    "            table += make_table_element(mean_auc, textbf_auc, idx)\n",
    "            \n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        table = \" & KU & \"\n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(mean_prr, textbf_prr, idx)\n",
    "            \n",
    "        for idx in range(num, 2*num):\n",
    "            table += make_table_element(mean_auc, textbf_auc, idx)\n",
    "        print(table.rstrip(\"& \") + \" \\\\\\\\\")\n",
    "        \n",
    "        print(\"\\midrule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc2c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
