@article{CHERKASSKY2004113,
title = {Practical selection of SVM parameters and noise estimation for SVM regression},
journal = {Neural Networks},
volume = {17},
number = {1},
pages = {113-126},
year = {2004},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(03)00169-2},
url = {https://www.sciencedirect.com/science/article/pii/S0893608003001692},
author = {Vladimir Cherkassky and Yunqian Ma},
keywords = {Complexity control, Loss function, Parameter selection, Prediction accuracy, Support vector machine regression, VC theory},
abstract = {We investigate practical selection of hyper-parameters for support vector machines (SVM) regression (that is, ε-insensitive zone and regularization parameter C). The proposed methodology advocates analytic parameter selection directly from the training data, rather than re-sampling approaches commonly used in SVM applications. In particular, we describe a new analytical prescription for setting the value of insensitive zone ε, as a function of training sample size. Good generalization performance of the proposed parameter selection is demonstrated empirically using several low- and high-dimensional regression problems. Further, we point out the importance of Vapnik's ε-insensitive loss for regression problems with finite samples. To this end, we compare generalization performance of SVM regression (using proposed selection of ε-values) with regression using ‘least-modulus’ loss (ε=0) and standard squared loss. These comparisons indicate superior generalization performance of SVM regression under sparse sample settings, for various types of additive noise.}
}