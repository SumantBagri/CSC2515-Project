\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage[final]{neurips}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{calc}
\usepackage{bbm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Investigating Uncertainty in Ensemble Methods}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Gurman Bhullar \\
  Department of Computer Science \\
  University of Toronto \\
  \texttt{gbhullar@cs.toronto.edu} \\
  %% examples of more authors
  \And
  Rajesh Marudhachalam \\
  Department of Computer Science \\
  University of Toronto \\
  \texttt{rajesh1804@cs.toronto.edu} \\
  \And
  Sarah Hindawi \\
  Department of Computer Science \\
  University of Toronto \\
  \texttt{shindawi@cs.toronto.edu} \\
  \And
  Sumant Bagri \\
  Department of Computer Science \\
  University of Toronto \\
  \texttt{sbagri@cs.toronto.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
\center{TBD...}
\end{abstract}


\section{Introduction}


\section{Motivation}


\section{Related Work}


\section{Theoretical Analysis}

\subsection{Quantifying Uncertainty}

In the domain of Bayesian inference for probabilistic models, given a dataset $\mathcal{D} = \{ x^{(i)}, y^{(i)}\}_{i=1}^N$, we try to model the parameters $\theta$ by estimating its posterior distribution using Bayes' rule:
\begin{align}
	\mathbb{P}(\theta|\mathcal{D}) = \frac{\mathbb{P}(\mathcal{D} | \theta)\mathbb{P}(\theta)}{\mathbb{P}(\mathcal{D})}
\end{align}

A given ensemble of models $\{ \mathbb{P}(y|x; \theta^{(m)}) \}_{m=1}^M$ is assumed to have been generated from a known (or in most cases a close approximation) of a Bayesian posterior $\mathbb{P}(\theta|\mathcal{D})$. The predictive posterior of the ensemble is then defined as follows:
\begin{align}
	\begin{split}
		\mathbb{P}(y|x, \mathcal{D}) &= \mathbb{E}_{\mathbb{P}(\theta|\mathcal{D})} \left[ \mathbb{P}(y|x,\theta) \right] \\
		\text{where, the sample mean}  &\text{estimator is used as an approximation for the expectation} \\
		\mathbb{P}(y|x, \mathcal{D}) &\simeq \frac{1}{M} \sum\limits_{m=1}^M \mathbb{P}(y|x; \theta^{(m)}) \\
	\end{split}
\end{align}

Uncertainty from the predictive posterior of the ensemble can be due to two factors: 1) aleatoric (data) uncertainty or 2) epistemic (knowledge) uncertainty. We can write the total uncertainty as a sum of aleatoric and epistemic uncertainties as shown below,
\begin{align}
	\text{Total Uncertainty = Data Uncertainty + Knowledge Uncertainty}
\end{align}

We define the total uncertainty as the entropy in the predictive posterior. This is defined below,
\begin{align}
	\mathcal{H} \left[ \mathbb{P}(y|x, \mathcal{D}) \right] = \mathbb{E}_{\mathbb{P}(y|x,\mathcal{D})} \left[ - \log{\mathbb{P}(y|x,\mathcal{D})} \right]
\end{align}

Now, the data uncertainty for a generalized predictor can be visualized as the expected prediction uncertainty of each model in the ensemble. Mathematically, this can be written as the expected entropy of each model in the ensemble as below,
\begin{align}
	\text{Expected Data Uncertainty} = \mathbb{E}_{\mathbb{P}(\theta|\mathcal{D})} \left[ \mathcal{H}(\mathbb{P}(y|x,\theta)) \right]
\end{align}

Therefore, we can obtain the knowledge uncertainty by rearranging (3)
\begin{align}
	\begin{split}
		\text{Knowledge Uncertainty} &= \text{Total Uncertainty} - \text{Data Uncertainty}\\
		\text{Knowledge Uncertainty} &\simeq \text{Total Uncertainty} - \text{Expected Data Uncertainty}\\
		\text{Knowledge Uncertainty} &\simeq \mathcal{H} \left[ \mathbb{P}(y|x, \mathcal{D}) \right] - \mathbb{E}_{\mathbb{P}(\theta|\mathcal{D})} \left[ \mathcal{H}(\mathbb{P}(y|x,\theta)) \right]\\
		\text{this is precisely } &\text{the information gain from knowing the data}\\
		\mathcal{I}\left[ y, \theta | x, \mathcal{D} \right] &= \mathcal{H} \left[ \frac{1}{M} \sum\limits_{m=1}^M \mathbb{P}(y|x,\theta^{(m)} \right] - \frac{1}{M} \sum\limits_{m=1}^M \mathcal{H}\left[ \mathbb{P}(y|x,\theta) \right]
	\end{split}
\end{align}

However, we can use the above formulation only for classification tasks where predictions are actually indicative of probabilities. For regression tasks, the predictions are point values and we cannot use them as probabilistic estimates for a model. For regression tasks, uncertainty is estimated using total variation which is defined below,
\begin{align}
		\mathbb{V}_{p(y|x, \mathcal{D})}[y] &= \mathbb{V}_{p(y|x, \mathcal{D})}[\mathbb{E}_{p(y|x, \mathcal{D})}[y]] + \mathbb{P}_{p(\theta|\mathcal{D})}[\mathbb{V}_{p(y|x,\theta)}[y]] \\
		\mathbb{V}_{p(y|x, \mathcal{D})}[y] &\simeq \frac{1}{M} \sum\limits_{m=1}^M \left[ \left( \sum\limits_{m-1}^M \frac{\mu_m}{M} \right) - \mu_m \right]^2 + \frac{1}{M} \sum\limits_{m=1}^M \sigma_m^2 \quad , \quad \{ \mu_m, \sigma_m \} = f(x; \theta^{(m)})
\end{align}

The parameters, $\{ \mu_m , \sigma_m \}$ are estimated by the training algorithm that models a normal distribution over the target $y$ given features $x$ by optimizing the negative log likelihood for $\mathbb{P}(y|x, \theta)$. This is mathematically presented as follows,
\begin{align}
	\begin{split}
		F^{(t)}(x) &: X \to \mathbb{R}^2 \Rightarrow \left\{ \mu_m^{(t)},\sigma_m^{(t)} \right\} \\
		\text{such} &\text{ that, } \\
		\mathbb{P}(y|x,\theta^{(t)}) &= \mathcal{N}\left( y|\mu_m^{(t)},\sigma_m^{(t)} \right), \text{ where,} \\
		\hat{\theta}^{(t)} &= \arg \min_{\theta} \mathbb{E}_{\mathcal{D}}[-\log{p(y|x, \theta)}] =  \arg \min_{\theta} \left\{ - \frac{1}{N} \sum\limits_{i=1}^N \log{\mathbb{P}(y^{(i)}|x^{(i)},\theta)} \right\} \\
	\end{split}
\end{align}

Using these uncertainty measures, we can formulate a thresholding-based binary classification problem where the predictor/detector $\mathcal{I}_T(x)$ takes in a new input $\textbf{x}$ and assigns a label 1 (uncertain prediction) if the uncertainty measure $\mathcal{H}(x)$ is above a threshold $T$ and label 0 (confident prediction) otherwise. This is presented, mathematically, as below,
\begin{align}
	\mathcal{I}_T(x) = 	\left\{
		\begin{array}{ll}
			1  & ,\mathcal{H}(x) > T\\
			0  & ,\mathcal{H}(x) \le T 
		\end{array}
	\right.
\end{align}

\subsection{Gradient Boosted Decision Trees (GBDT)}

Given a dataset $\mathcal{D}$ and a loss function $L : \mathbb{R}^2 \to \mathbb{R}$, the gradient boosting algorithm iteratively constructs a model $F : X \to \mathbb{R}$ to minimize the empirical risk $\mathcal{L}(F|\mathcal{D}) = \mathbb{E}_{\mathcal{D}} [L(F(x),y)]$ \citet{friedman}. A weak learner $h^{(t)}$ is chosen at each time-step $(t)$ from a family of functions $\mathcal{H}$ and the model is updated as follows,
\begin{align}
	F^{(t)}(x) &= F^{(t-1)}(x) + \epsilon \cdot h^{(t)}(x) \\
	\text{such} &\text{ that, } \notag \\
	h^{(t)} &= \arg \min_{h \in \mathcal{H}} \mathbb{E}_{\mathcal{D}} \left[ \left( - \left. \frac{\partial{\mathcal{L}(y,s)}}{\partial{s}} \right|_{s = F^{(t-1)}(x)} - h(x) \right)^2 \right]
\end{align}
Here, $\epsilon$ is the learning rate for the model $F$. For the GBDT ensemble the weak learners are decision trees defined using a set of parameters $\theta^{(t)}$ such that $h(x, \phi^{(t)}) = \sum\limits_{j=1}^{d} \phi_{j}^{(t)} \textbf{1}_{x \in R_{j}}$. The ensemble is constructed by summing over all the decision trees

\subsection{Generating Ensembles}

One of the most important requirements for calculating uncertainty is ensemble models is that each model in the ensemble should be sampled from a posterior distribution. $\mathbb{P}(\theta | \mathcal{D})$. Stochastic Gradient Boosting (SGB) and Stochastic Gradient Langevin Boosting (SGLB) 

\textbf{SGB ensembles}

SGB ensemble of GBDTs are generated by subsampling the dataset $\mathcal{D}$ at each iteration  of training. A randomly subsampled dataset $\mathcal{D'} \subset \mathcal{D}$ is generated either using bootstrap or uniformly randomly. The fraction of the dataset sampled is called the \textit{sample rate}. The sampling is done independently resulting in an ensemble of independent models $\{ \theta^{(m)} \}_{m=1}^M$.

The model updates are done using (10) and (11).

\textbf{SGLB ensembles}

SGLB combines gradient boosting along with Langevin dynamics \citet{raginsky} to achieve convergence to global optimum even for non-convex loss functions. The algorithm has two main differences compared to SGB \cite{sglb}
\begin{align}
	\text{First, } &\text{Gaussian noise is added to the SE calculation} \notag \\
	h^{(t)} &= \arg \min_{h \in \mathcal{H}} \mathbb{E}_{\mathcal{D}} \left[ \left( \left. \frac{\partial{\mathcal{L}(y,s)}}{\partial{s}} \right|_{s = F^{(t-1)}(x)} - h(x) + v \right)^2 \right], v \sim \mathcal{N}\left( 0, \frac{2}{\beta_{\epsilon}} \textbf{I}\right) \\
	\text{Second, } & \notag \\
	F^{(t)}(x) &= (1-\gamma\epsilon)F^{(t-1)}(x) + \epsilon \cdot h^{(t)}(x, \phi^{(t)})
\end{align}
Here, $\beta$ is called the diffusion temperature and helps in controlling the random exploration induced by $v$. The random exploration is what helps SGLB achieve global optimum

\textbf{Optimizers and Training Metrics}

\textbf{\underline{Classification}}

The LogLoss optimizer is used for training the classification models using SGB and SGLB algorithms. This is defined as follows,
\begin{align}
	\mathcal{L}\left(y,\{ \mathbb{P}(y|x; \theta^{(m)}) \}_{m=1}^M \right)_{LogLoss} = - \frac{\sum\limits_{m=1}^M \left\{ w_m\left(t\log\left[\mathbb{P}(y|x; \theta^{(m)})\right] + (1-t)\log\left[ 1 - \mathbb{P}(y|x; \theta^{(m)}\right]\right) \right\}}{\sum\limits_{i=m}^M w_m}
\end{align}

The $0-1$ Loss is used as the evaluation metric during training and testing

\textbf{\underline{Regression}}

For regression, the RMSEWithUncertainty optimizer is used. This is described below,
\begin{align}
	\mathcal{L}\left(y,\{ \mathbb{P}(y|x; \theta^{(m)}) \}_{m=1}^M \right)_{\text{RMSEWithUncertainty}} =-\frac{\sum\limits_{i=1}^N \left\{ w_m \log\left[ \mathcal{N}(y|a_{m,0},e^{2a_{m,1}}) \right] \right\}}{\sum\limits_{m=1}^M w_m} \notag
\end{align}
\begin{align}
	\mathcal{L}_{\text{RMSEWithUncertainty}} = \frac{1}{2}\log(2\pi) + \frac{\sum\limits_{m=1}^M \left\{ w_m\left(a_{m,0} + \frac{1}{2}e^{-2a_{m,1}}(t-a_{m,0})^2\right) \right\}}{\sum\limits_{m=1}^M w_m}
\end{align}

Here $a_{m,0}$ is the mean prediction ($\mu_m$) and $a_{m,1}$ is the $\log(\sigma_m)$ prediction such that the model $m$ is sampled from $\mathcal{N}(y|\mu_m,\sigma_m^2) = \frac{1}{\sqrt{2\pi\sigma_m^2}}\exp\left( -\frac{(y-\mu_m)^2}{2\sigma^2} \right)$

The RMSE score is used as the evaluation metric during training and testing

\subsection{Metrics for evaluating uncertainty}

It is important to know whether the prediction uncertainty from an ensemble (as computed using methods stated above) is able to provide any meaningful insights on predictions: Whether the model will make an error in prediction or whether it will classify it correctly.

\textbf{AUC-ROC (for out-of-domain datasets)}

OOD detection is done using a dataset consisting of both in-domain ($\mathcal{I} \in \mathcal{D}_{in}$) and out-of-domain ($\mathcal{O} \in \mathcal{D}_{out}$) datapoints. Points belonging to $\mathcal{I}$ are labeled as 1 while points in $\mathcal{O}$ are labeled as 0. Therefore the model for the ensemble becomes $F : X \to \{0,1\}$. The model is trained on the training dataset and its predictions $\mathcal{P}$ are collected from the test dataset ($\mathcal{D}_{test}$) with true labels $\mathcal{T}$. The strategy is to then evaluate the \textit{true positive rate (tpr)} and the \textit{false positive rates (fpr)} as follows
\begin{align}
	\begin{split}
		tpr = \frac{\sum_{x \in \mathcal{D}_{test}} \mathcal{I}_T(x| \mathcal{T}(x) = 0)}{\sum_{x \in \mathcal{D}_{test}} \mathbbm{1}\left\{ \mathcal{T}(x) = 0\right\}} \\
		fpr  = \frac{\sum_{x \in \mathcal{D}_{test}} \mathcal{I}_T(x|\mathcal{T}(x) = 1)}{\sum_{x \in \mathcal{D}_{test}} \mathbbm{1}\left\{ \mathcal{T}(x) = 1\right\}} \\
	\end{split}
\end{align}
where $\mathcal{I}_T$ is the detector/predictor as defined in (4.1)

Subsequently, we plot the ROC curvce as: \text{tpr} vs \text{fpr}, and compute the area under the curve (AUC). The higher the value of AUC-ROC the better is the out-of-domain detection.

\textbf{Prediction-Rejection Ratio (PRR)}

For a classification problem, the measure of uncertainty provides a means to label model predictions as certain or uncertain. Ideally, the goal is to be able to detect all the inputs which the model has misclassified with a label of "uncertain prediction". If the detector $\mathcal{I}_T$ (for a given uncertaintly threshold $T$), is able to achieve this then the model can choose not to provide any prediction for these inputs, or pass them over to an "oracle" (human) as "rejected-for-prediction". This can be visualized as a rejection curve $\mathcal{R}$ on the percentage of data points in $f * N(\mathcal{D}_{test}) : f \in [0,1]$

The base error ($e_{base}$) in classification is defined as the error after the model($F$) (and the detector $\mathcal{I}_T$) have gone through the whole test dataset $\mathcal{D}_{test}$. We consider two boundary cases for the detector $\mathcal{I}_T$,
\begin{itemize}
	\item[\textbf{\underline{Case 1:}}] Detector is completely unreliable always producing useless estimates for uncertainty ($\mathcal{R}_{rand}$)
	\item[\textbf{\underline{Case 2:}}] Detector is perfect and always bigger for a misclassification than for a correct classification. In this case, the curve will represent the oracle curve (where all uncertain misclassifications are given to the oracle to classify) ($\mathcal{R}_{orc}$)
\end{itemize}
Any other detector would lie between the above two boundary cases and is denoted as $\mathcal{R}_{uns}$

$\mathcal{R}_{rand}$ is a straight line from $e_{base}$ at 0\% rejection to 0 at 100\% rejection as the rejections are random:
\begin{align}
	\mathcal{R}_{rand} - 0 &= \frac{e_{base} - 0}{0 - 1} \times (f - 1) \\
	\mathcal{R}_{rand} &= (1 - f) \times e_{base}
\end{align}

$\mathcal{R}_{rand}$ is a straight line from $e_{base}$ at 0\% rejection to 0 at the percentage of misclassification $\left( N_{misclassify} = \sum_{i=1}^{N(\mathcal{D}_{test})} \mathbbm{1} [y \ne F(x)] \Rightarrow f' = \frac{\sum_{i=1}^{N(\mathcal{D}_{test})} \mathbbm{1} [y \ne F(x)]}{N(\mathcal{D}_{test})} \right)$ as the "perfect" detector will have no need to reject any correctly classfied datapoints:
\begin{align}
	\mathcal{R}_{orc} - 0 &= \frac{e_{base} - 0}{0 - f'} \times (f - f') \\
	\mathcal{R}_{rand} &= \left\{
		\begin{array}{ll}
			e_{base} \times (1 - \frac{f}{f'})  & ,f < f' \\ \\
			0  & ,1 \ge f \ge f' 
		\end{array}
	\right. 
\end{align}

The quality of th rejection curve is assesed by considering the \textit{ratio} of the area between $\mathcal{R}_{uns}$ and $\mathcal{R}_{random}$ and is called the \textit{Prediction-Rejection-Ratio (PRR)} as shown below,
\begin{align}
	PRR = \frac{AUC({R}_{random}) - AUC({R}_{orc})}{AUC({R}_{uns}) - AUC({R}_{orc})}
\end{align}

A higher value of PRR indicates that the detector is closer to the oracle in terms of classification error rate and is more certain about the misclassified labels.

\section{Experimental Setup}

\section{Results and Discussion}

\section{Conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{report}  % .bib


\end{document}